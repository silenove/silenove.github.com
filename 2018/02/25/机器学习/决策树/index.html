<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-EN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.0.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="机器学习,决策树," />


<meta name="description" content="决策树 决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。 决策树学习通常包括">
<meta name="keywords" content="机器学习,决策树">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://yoursite.com/2018/02/25/机器学习/决策树/index.html">
<meta property="og:site_name" content="silenove blogs">
<meta property="og:description" content="决策树 决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。 决策树学习通常包括">
<meta property="og:locale" content="zh-EN">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20上午10.01.38.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20上午10.36.56.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20下午9.03.03.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20下午10.50.09.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-23%20下午4.26.28.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-23%20上午10.59.29.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-23%20上午10.47.08.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-24%20上午10.47.13.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-24%20上午10.50.00.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-25%20下午3.59.08.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-25%20下午4.13.34.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-25%20下午4.19.43.png">
<meta property="og:updated_time" content="2018-02-25T08:22:48.803Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:description" content="决策树 决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。 学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。 决策树学习通常包括">
<meta name="twitter:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20上午10.01.38.png">






  <link rel="canonical" href="http://yoursite.com/2018/02/25/机器学习/决策树/"/>


  <title>决策树 | silenove blogs</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-EN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">silenove blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">沿路旅程如歌褪变</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Startseite</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archiv</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/25/机器学习/决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="silen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="silenove blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-25T16:21:22+08:00">2018-02-25</time>
            

            
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="决策树">决策树</h1>
<p>决策树（decision tree）是一种基本的分类与回归方法。决策树模型呈树状结构，在分类问题中，表示基于特征对实例进行分类的过程。他可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。<br>
学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型；预测时，对新的数据，利用决策树模型进行分类。<br>
决策树学习通常包括3个过程：特征选择、决策树的生成、决策树的修剪。</p>
<h2 id="决策树模型与学习">1. 决策树模型与学习</h2>
<h3 id="决策树模型">1.1. 决策树模型</h3>
<p><strong>决策树</strong>：分类决策树是一种描述对实例进行分类的树形结构。决策树由节点和有向边组成。节点有两种类型：内部节点（internal node）和叶节点（leaf node），内部节点表示一个特征或属性，叶节点表示一个类。<br>
用决策树分类，从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点；此时，每一个子节点对应着该特征的一个取值。如此递归地对实例进行测试并匹配，直至达到叶节点，最后将实例分到叶节点的类中。<br>
决策树示意图，圆和方框分别表示内部节点和叶节点： <img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20上午10.01.38.png"></p>
<h3 id="决策树学习">1.2. 决策树学习</h3>
<p>决策树学习本质上是从训练数据集中归纳出一组分类规则，与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型有无穷多个，选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。<br>
决策树学习用损失函数表示这一目标，决策树学习的损失函数通常是正则化的极大似然函数，决策树学习的策略是以损失函数为目标函数的最小化。<br>
当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题。从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优问题，得到的决策树是次优的。<br>
决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。<br>
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20上午10.36.56.png"></p>
<p>在决策树的基本算法中，有三种情形会导致递归返回：</p>
<ol style="list-style-type: decimal">
<li>当前节点包含的样本全属于同一类别，无需划分；</li>
<li>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。此时，我们把当前节点标记为叶节点，并将其类别设定为该节点所含样本最多的类别；</li>
<li>当前节点包含的样本集合为空，不能划分。我们同样把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别。</li>
</ol>
<p>上述方法生成的决策树可能对训练数据有很好的划分能力，但对未知的测试数据却未必有很好的划分能力，即可能发生过拟合现象。这时需要对已生成的决策树自下而上进行剪枝，将树变得简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶节点，使其回退到父节点，甚至更高的节点，然后将父节点或更高的节点改为新的叶节点。<br>
如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，留下对训练数据有足够分类能力的特征。<br>
决策树的生成对应模型的局部选择，决策树的剪枝则考虑全局最优。</p>
<h2 id="特征选择">2. 特征选择</h2>
<p>决策树学习的关键是如何选择最优的划分属性，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高。</p>
<h3 id="信息增益">2.1. 信息增益</h3>
<p>首先给出熵和条件熵的定义。<br>
在信息论和概率统计中，熵（entropy）是表示随机变量不确定性的度量。设<span class="math inline">\(X\)</span>是一个取有限个值的离散随机变量，其概率分布为 <span class="math display">\[P(X=x_i) = p_i, \quad i=1,2,\cdots,n\]</span> 则随机变量<span class="math inline">\(X\)</span>的熵定义为 <span class="math display">\[H(X) = - \sum_{i=1}^n p_i \log p_i\]</span> 在上式中，若<span class="math inline">\(p_i = 0\)</span>，则定义<span class="math inline">\(0 \log 0 = 0\)</span>。通常，上式中的对数以2为底或以e为底，这时熵的单位分别称为比特（bit）或纳特（nat）。由定义可知，熵只依赖于<span class="math inline">\(X\)</span>的分布，而与<span class="math inline">\(X\)</span>的取值无关，所以也可将<span class="math inline">\(X\)</span>的熵记作<span class="math inline">\(H(p)\)</span>，即 <span class="math display">\[H(p) = - \sum_{i=1}^n p_i \log p_i\]</span> 熵越大，随机变量的不确定性就越大，从定义可验证 <span class="math display">\[0 \leq H(p) \leq \log n\]</span> 当随机变量只取两个值，例如1，0时，即<span class="math inline">\(X\)</span>的分布为 <span class="math display">\[P(X=1)=p, \quad P(X=0)=1-p, \quad 0 \leq p \leq 1\]</span> 熵为 <span class="math display">\[H(p) = -p \log_2 p - (1-p) \log_2 (1-p)\]</span> 这时，熵<span class="math inline">\(H(p)\)</span>随概率<span class="math inline">\(p\)</span>变化的曲线如下图所示（单位为比特）： <img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20下午9.03.03.png"></p>
<p>当<span class="math inline">\(p=0\)</span>或<span class="math inline">\(p=1\)</span>时<span class="math inline">\(H(p)=0\)</span>，随机变量完全没有不确定性，当<span class="math inline">\(p=0.5\)</span>时，<span class="math inline">\(H(p)=1\)</span>，熵取值最大，随机变量不确定性最大。<br>
设有随机变量<span class="math inline">\((X,Y)\)</span>，其联合概率分布为 <span class="math display">\[P(X=x_i,Y=y_i) = p_{ij}, \quad i=1,2,\cdots,n; \quad j=1,2,\cdots,m\]</span> 条件熵<span class="math inline">\(H(Y|X)\)</span>表示在已知随机变量<span class="math inline">\(X\)</span>的条件下随机变量<span class="math inline">\(Y\)</span>的不确定性。随机变量<span class="math inline">\(X\)</span>给定的条件下随机变量<span class="math inline">\(Y\)</span>的条件熵（conditional entropy）<span class="math inline">\(H(Y|X)\)</span>，定义为<span class="math inline">\(X\)</span>给定条件下<span class="math inline">\(Y\)</span>的条件概率分布的熵对<span class="math inline">\(X\)</span>的数学期望 <span class="math display">\[H(Y|X) = \sum_{i=1}^n p_i H(Y|X=x_i)\]</span> 其中<span class="math inline">\(p_i = P(X=x_i), \quad i = 1,2,\cdots,n\)</span>。<br>
当熵和条件熵的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。如果有0概率，令<span class="math inline">\(0 \log 0 = 0\)</span>。<br>
信息增益（information gain）表示得到特征<span class="math inline">\(X\)</span>的信息而使得类<span class="math inline">\(Y\)</span>的信息不确定性减少的程度。<br>
<strong>信息增益</strong>：特征<span class="math inline">\(A\)</span>对训练数据集<span class="math inline">\(D\)</span>的信息增益<span class="math inline">\(g(D,A)\)</span>，定义为集合<span class="math inline">\(D\)</span>的经验熵<span class="math inline">\(H(D)\)</span>与特征<span class="math inline">\(A\)</span>给定条件下<span class="math inline">\(D\)</span>的经验条件熵<span class="math inline">\(H(D|A)\)</span>之差，即 <span class="math display">\[g(D,A) = H(D) - H(D|A)\]</span> 通常，熵<span class="math inline">\(H(Y)\)</span>与条件熵<span class="math inline">\(H(Y|X)\)</span>之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。<br>
决策树学习应用信息增益准则选择特征。给定训练数据集<span class="math inline">\(D\)</span>和特征<span class="math inline">\(A\)</span>，经验熵<span class="math inline">\(H(D)\)</span>表示对数据集<span class="math inline">\(D\)</span>进行分类的不确定性。而经验条件熵<span class="math inline">\(H(D|A)\)</span>表示在特征<span class="math inline">\(A\)</span>给定的条件下对数据集<span class="math inline">\(D\)</span>进行分类的不确定性。它们的差为信息增益，表示由于特征<span class="math inline">\(A\)</span>而使得对数据集<span class="math inline">\(D\)</span>的分类的不确定性减少的程度。显然，对于数据集<span class="math inline">\(D\)</span>而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。<br>
根据信息增益准则的特征选择方法：对训练数据集（或子集）<span class="math inline">\(D\)</span>，计算其每个特征的信息增益，比较它们的大小，选择信息增益最大的特征。<br>
<strong>信息增益的算法</strong>： 输入：训练数据集<span class="math inline">\(D\)</span>和特征<span class="math inline">\(A\)</span>； 输出：特征<span class="math inline">\(A\)</span>对训练数据集<span class="math inline">\(D\)</span>的信息增益<span class="math inline">\(g(D,A)\)</span>。<br>
（1）计算数据集<span class="math inline">\(D\)</span>的经验熵<span class="math inline">\(H(D)\)</span> <span class="math display">\[H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|}\]</span> （2）计算特征<span class="math inline">\(A\)</span>对数据集<span class="math inline">\(D\)</span>的经验条件熵<span class="math inline">\(H(D|A)\)</span> <span class="math display">\[H(D|A)= \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) = - \sum_{i=1}^n \frac{|D_i|}{|D|} \sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}\]</span> （3）计算信息增益 <span class="math display">\[g(D,A) = H(D) - H(D|A)\]</span></p>
<h3 id="信息增益比">2.2. 信息增益比</h3>
<p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正，这也是特征选择的另一准则。<br>
<strong>信息增益比</strong>：特征<span class="math inline">\(A\)</span>对训练数据集<span class="math inline">\(D\)</span>的信息增益比<span class="math inline">\(g_R(D,A)\)</span>定义为其信息增益<span class="math inline">\(g(D,A)\)</span>与训练数据集<span class="math inline">\(D\)</span>关于特征<span class="math inline">\(A\)</span>的值的熵<span class="math inline">\(H_A(D)\)</span>之比，即 <span class="math display">\[g_R(D,A) = \frac{g(D,A)}{H_A(D)}\]</span> 其中，<span class="math inline">\(H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|}\)</span>，<span class="math inline">\(n\)</span>是特征<span class="math inline">\(A\)</span>取值的个数。</p>
<h3 id="基尼指数">2.3. 基尼指数</h3>
<p><strong>基尼指数</strong>：分类问题中，假设有<span class="math inline">\(K\)</span>个类，样本点属于第<span class="math inline">\(k\)</span>类的概率为<span class="math inline">\(p_k\)</span>，则概率分布的基尼指数定义为 <span class="math display">\[Gini(p) = \sum_{k=1}^K p_k (1 - p_k) = 1 - \sum_{k=1}^K p_k^2\]</span> 对于二分类问题，若样本点属于第1个类的概率是<span class="math inline">\(p\)</span>，则概率分布的基尼指数为 <span class="math display">\[Gini(p) = 2p(1-p)\]</span> 对于给定的样本集合<span class="math inline">\(D\)</span>，其基尼指数为 <span class="math display">\[Gini(D) = 1 - \sum_{i=1}^K (\frac{|C_k|}{|D|})^2\]</span> 上式中，<span class="math inline">\(C_k\)</span>是<span class="math inline">\(D\)</span>中属于第<span class="math inline">\(k\)</span>类的样本子集，<span class="math inline">\(K\)</span>是类的个数。<br>
如果样本集合<span class="math inline">\(D\)</span>根据特征<span class="math inline">\(A\)</span>是否取某一可能值<span class="math inline">\(a\)</span>被分割成<span class="math inline">\(D_1\)</span>和<span class="math inline">\(D_2\)</span>两部分，即 <span class="math display">\[D_1 = \{(x,y) \in D | A(x) = a \}, \quad D_2 = D - D_1\]</span> 则在特征<span class="math inline">\(A\)</span>的条件下，集合<span class="math inline">\(D\)</span>的基尼指数定义为 <span class="math display">\[Gini(D,A) = \frac{|D_i|}{|D|} Gini(D_1) + \frac{|D_2|}{|D|} Gini(D_2)\]</span> 基尼指数<span class="math inline">\(Gini(D)\)</span>表示集合<span class="math inline">\(D\)</span>的不确定性，基尼指数<span class="math inline">\(Gini(D,A)\)</span>表示经<span class="math inline">\(A=a\)</span>分割后集合<span class="math inline">\(D\)</span>的不确定性。基尼指数越大，样本集合的不确定性也就越大，这一点与熵相似。<br>
下图显示二类问题中基尼指数<span class="math inline">\(Gini(p)\)</span>、熵（单位比特）之半<span class="math inline">\(\frac{1}{2}H(p)\)</span>和分类误差率的关系。横坐标表示概率<span class="math inline">\(p\)</span>，纵坐标表示损失。<br>
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-22%20下午10.50.09.png"></p>
<h2 id="决策树剪枝">3. 决策树剪枝</h2>
<p>决策树生成算法递归地产生决策树，这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没那么准确，即出现了过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决此问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。<br>
在决策树学习中将已生成的树进行简化的过程成为剪枝（pruning）。剪枝从已生成的树上裁掉一些子树或叶节点，并将其根节点或父节点作为新的叶节点，建华分类树模型。<br>
决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。设树<span class="math inline">\(T\)</span>的叶节点个数为<span class="math inline">\(|T|\)</span>，<span class="math inline">\(t\)</span>是树<span class="math inline">\(T\)</span>的叶节点，该叶节点有<span class="math inline">\(N_t\)</span>个样本点，其中<span class="math inline">\(k\)</span>类的样本点有<span class="math inline">\(N_{tk}\)</span>个，<span class="math inline">\(k=1,2,\cdots,K\)</span>，<span class="math inline">\(H_t(T)\)</span>为叶节点<span class="math inline">\(t\)</span>上的经验熵，<span class="math inline">\(\alpha \geq 0\)</span>为参数，则决策树学习的损失函数可以定义为 <span class="math display">\[C_\alpha (T) = \sum_{t=1}^{|T|} N_t H_t(T) + \alpha |T|\]</span> 其中经验熵为 <span class="math display">\[H_t(T) = - \sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}\]</span> 损失函数中，有段第一项记为 <span class="math display">\[C(T) = \sum_{t=1}^{|T|} N_t H_t(T) = - \sum_{t=1}^{|T|} \sum_{k=1}^K N_{tk} \log \frac{N_{tk}}{N_t}\]</span> 此时损失函数可写为 <span class="math display">\[C_\alpha(T) = C(T) + \alpha |T|\]</span> 上式中，<span class="math inline">\(C(T)\)</span>表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，<span class="math inline">\(|T|\)</span>表示模型复杂度，参数<span class="math inline">\(\alpha \geq 0\)</span>控制两者之间的影响。较大的<span class="math inline">\(\alpha\)</span>促使选择较简单的模型，较小的<span class="math inline">\(\alpha\)</span>促使选择较复杂的模型。<span class="math inline">\(\alpha = 0\)</span>意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。<br>
当<span class="math inline">\(\alpha\)</span>值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示对两者的平衡。<br>
决策树生成只考虑了通过提高信息增益（信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型的复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。<br>
利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-23%20下午4.26.28.png">

</div>
<p>公式（5.15）只需考虑两个树的损失函数的差，其计算可以在局部进行。所以，决策树的剪枝算法可以有一种动态规划的算法实现。</p>
<p>周志华的《机器学习》一书中对于决策树剪枝策略的介绍分为预剪枝和后剪枝。预剪枝是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点；后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若该节点对应的子树替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点。其中泛化能力是用验证集上的准确度进行估计。</p>
<h2 id="连续值与缺失值">4. 连续值与缺失值</h2>
<h3 id="连续值处理">4.1. 连续值处理</h3>
<p>现实学习任务中常会遇到连续属性，由于连续属性的可取值数目不在有限，因此，不能直接根据连续属性的可取值来对节点进行划分，此时需要借助连续属性离散化技术。最简单的策略是采用二分法对连续属性进行处理，这也是C4.5决策树算法中采用的机制。<br>
给定样本集<span class="math inline">\(D\)</span>和连续属性<span class="math inline">\(a\)</span>，假定<span class="math inline">\(a\)</span>在<span class="math inline">\(D\)</span>上出现了<span class="math inline">\(n\)</span>个不同的取值，将这些值从小到大进行排序，记为<span class="math inline">\(\{a^1, a^2, \cdots, a^n\}\)</span>。基于划分点<span class="math inline">\(t\)</span>可将<span class="math inline">\(D\)</span>分为子集<span class="math inline">\(D_t^-\)</span>和<span class="math inline">\(D_t^+\)</span>，其中<span class="math inline">\(D_t^-\)</span>包含那些在属性<span class="math inline">\(a\)</span>上取值不大于<span class="math inline">\(t\)</span>的样本，而<span class="math inline">\(D_t^+\)</span>则包含那些在属性<span class="math inline">\(a\)</span>上取值大于<span class="math inline">\(t\)</span>的样本。显然，对相邻的属性取值<span class="math inline">\(a^i\)</span>与<span class="math inline">\(a^{i+1}\)</span>来说，<span class="math inline">\(t\)</span>在区间<span class="math inline">\([a^i,a^{i+1})\)</span>中取任意值所产生的划分结果相同。因此，对连续属性<span class="math inline">\(a\)</span>，可考察包含<span class="math inline">\(n-1\)</span>个元素的候选划分点集合 <span class="math display">\[T_a = \{ \frac{a^i + a^{i+1}}{2} | 1 \leq i \leq n-1\}\]</span> 即把区间<span class="math inline">\([a^i,a^{i+1})\)</span>的中位点<span class="math inline">\(\frac{a^i + a^{i+1}}{2}\)</span>作为候选划分点。然后就可以像离散属性值一样考察这些划分点，选取最优的划分点进行样本集合的划分： <span class="math display">\[\begin{align}
    Gain(D,a) &amp;= \max_{t \in T_a} Gain(D,a,t) \nonumber \\
    &amp;= \max_{t \in T_a} (H(D) - \sum_{\lambda \in \{-,+\}} \frac{|D_t^\lambda|}{|D|} H(D_t^\lambda)) \nonumber 
\end{align}\]</span> 其中<span class="math inline">\(Gain(D,a,t)\)</span>是样本集<span class="math inline">\(D\)</span>基于划分点<span class="math inline">\(t\)</span>二分后的信息增益，于是就选择使<span class="math inline">\(Gain(D,a,t)\)</span>最大化的划分点。<br>
<strong>注意</strong>：与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。例如，在西瓜问题中，在父节点使用了“密度<span class="math inline">\(\leq 0.381\)</span>”，在子节点上仍然可以使用“密度<span class="math inline">\(\leq 0.294\)</span>”。</p>
<h3 id="缺失值处理">4.2. 缺失值处理</h3>
<p>现实任务中常会遇到不完整样本，即样本的某些属性值缺失，尤其是在属性数目较多的情况下，往往会有大量样本出现缺失值。如果简单地放弃不完整样本，仅使用无缺失值的样本进行学习，显然是对数据信息的极大浪费。<br>
在面对缺失值问题时，我们需要解决两个问题：</p>
<ol style="list-style-type: decimal">
<li>如何在属性值缺失的情况下进行划分属性选择？</li>
<li>给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</li>
</ol>
<p>给定训练集<span class="math inline">\(D\)</span>和属性<span class="math inline">\(a\)</span>，令<span class="math inline">\(\tilde{D}\)</span>表示<span class="math inline">\(D\)</span>中在属性<span class="math inline">\(a\)</span>上没有缺失值的样本子集。<br>
问题（1）：假定属性<span class="math inline">\(a\)</span>有<span class="math inline">\(V\)</span>个可取值<span class="math inline">\(\{a^1, a^2, \cdots, a^V\}\)</span>，令<span class="math inline">\(\tilde{D}^v\)</span>表示<span class="math inline">\(\tilde{D}\)</span>中在属性<span class="math inline">\(a\)</span>上取值为<span class="math inline">\(a^v\)</span>的样本子集，<span class="math inline">\(\tilde{D}_k\)</span>表示<span class="math inline">\(\tilde{D}\)</span>中属于第<span class="math inline">\(k\)</span>类（<span class="math inline">\(k=1,2,\cdots,|\gamma|\)</span>）的样本子集，显然有<span class="math inline">\(\tilde{D} = \cup_{k=1}^{|\gamma|} \tilde{D}_k\)</span>，<span class="math inline">\(\tilde{D} = \cup_{v=1}^V \tilde{D}^v\)</span>，假定为每个样本赋予一个权重<span class="math inline">\(w_{\pmb{x}}\)</span>，并定义 <span class="math display">\[\begin{align}
    \rho &amp;= \frac{\sum_{\pmb{x} \in \tilde{D}} w_{\pmb{x}}}{\sum_{\pmb{x} \in D} w_{\pmb{x}}} \nonumber \\
    \tilde{p}_k &amp;= \frac{\sum_{\pmb{x} \in \tilde{D}_k} w_{\pmb{x}}}{\sum_{\pmb{x} \in \tilde{D}} w_{\pmb{x}}} \quad (1 \leq k \leq |\gamma|) \nonumber \\
    \tilde{r}_v &amp;= \frac{\sum_{\pmb{x} \in \tilde{D}^v} w_{\pmb{x}}}{\sum_{\pmb{x} \in \tilde{D}} w_{\pmb{x}}} \quad (1 \leq v \leq V) \nonumber
\end{align}\]</span> 直观地看，对属性<span class="math inline">\(a\)</span>，<span class="math inline">\(\rho\)</span>表示无缺失值样本所占的比例，<span class="math inline">\(\tilde{p}_k\)</span>表示无缺失值样本中第<span class="math inline">\(k\)</span>类所占的比例，<span class="math inline">\(\tilde{r}_v\)</span>表示无缺失值样本中在属性<span class="math inline">\(a\)</span>上取值为<span class="math inline">\(a^v\)</span>的样本所占的比例，显然有<span class="math inline">\(\sum_{k=1}^{|\gamma|} \tilde{p}_k = 1, \sum_{v=1}^V \tilde{r}_v = 1\)</span>。<br>
基于上述定义，可将信息增益的计算式推广为 <span class="math display">\[\begin{align}
    Gain(D,a) &amp;= \rho \times Gain(\tilde{D},a) \nonumber \\
    &amp;= \rho \times (H(\tilde{D}) - \sum_{v=1}^V \tilde{r}_v H(\tilde{D}^v)) \nonumber 
\end{align}\]</span> 其中 <span class="math display">\[H(\tilde{D}) = - \sum_{k=1}^{|\gamma|} \tilde{p}_k \log_2 \tilde{p}_k\]</span></p>
<p>问题（2）：若样本<span class="math inline">\(\pmb{x}\)</span>在划分属性<span class="math inline">\(a\)</span>上的取值已知，则将<span class="math inline">\(\pmb{x}\)</span>划分入与其取值对应的子节点，且样本权值在子节点中保持为<span class="math inline">\(w_{\pmb{x}}\)</span>。若样本<span class="math inline">\(\pmb{x}\)</span>在划分属性<span class="math inline">\(a\)</span>上的取值未知，则将<span class="math inline">\(\pmb{x}\)</span>同时划入所有子节点，且样本权值在与属性值<span class="math inline">\(a^v\)</span>对应的子节点中调整为<span class="math inline">\(\tilde{r}_v · w_{\pmb{x}}\)</span>。直观来看，就是让同一样本以不同的概率划入到不同的子节点中。<br>
C4.5算法就是使用的上述解决方案。</p>
<h2 id="id3和c4.5算法实现">5. ID3和C4.5算法实现</h2>
<h3 id="id3算法">5.1. ID3算法</h3>
<p>ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归地构建决策树。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-23%20上午10.59.29.png">

</div>
<h3 id="c4.5算法">5.2. C4.5算法</h3>
<p>C4.5算法与ID3算法相似，只是在生成过程中，用信息增益比来选择特征。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-23%20上午10.47.08.png">

</div>
<h2 id="cart算法">6. CART算法</h2>
<p>分类与回归树（classification and regression tree，CART）是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。<br>
CART是在给定输入随机变量<span class="math inline">\(X\)</span>条件下输出随机变量<span class="math inline">\(Y\)</span>的条件概率分布的学习方法。CART假设决策树是二叉树，内部节点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。<br>
CART算法分为两步：</p>
<ol style="list-style-type: decimal">
<li>决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；</li>
<li>决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，用损失函数最小作为剪枝的标准。</li>
</ol>
<h3 id="cart生成">6.1. CART生成</h3>
<p>决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则。</p>
<h4 id="回归树">6.1.1. 回归树</h4>
<p>假设<span class="math inline">\(X\)</span>与<span class="math inline">\(Y\)</span>分别为输入和输出变量，并且<span class="math inline">\(Y\)</span>是连续变量，给定训练数据集： <span class="math display">\[D = \{(x_1,y_1), (x_2, y_2), \cdots, (x_N,y_N)\}\]</span> 一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为<span class="math inline">\(M\)</span>个单元<span class="math inline">\(R_1,R_2,\cdots,R_M\)</span>，并且在每个单元<span class="math inline">\(R_m\)</span>上有一个固定的输出值<span class="math inline">\(c_m\)</span>，于是回归树模型可表示为 <span class="math display">\[f(x) = \sum_{m=1}^M c_m I(x \in R_m)\]</span> 当输出空间的划分确定时，可以用平方误差<span class="math inline">\(\sum_{x_i \in R_m} (y_i - f(x_i))^2\)</span>来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元<span class="math inline">\(R_m\)</span>上的<span class="math inline">\(c_m\)</span>的最优值<span class="math inline">\(\hat{c}_m\)</span>是<span class="math inline">\(R_m\)</span>上的所有输入实例<span class="math inline">\(x_i\)</span>对应的输出<span class="math inline">\(y_i\)</span>的均值，即 <span class="math display">\[\hat{c}_m = ave(y_i|x_i \in R_m)\]</span> 问题是如何对输入空间进行划分。这里采用启发式的方法，选择第<span class="math inline">\(j\)</span>个变量<span class="math inline">\(x^{(j)}\)</span>和它的取值<span class="math inline">\(s\)</span>，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域： <span class="math display">\[R_1(j,s) = \{x|x^{(j)} \leq s \} \quad R_2(j,s) = \{x| x^{(j)} &gt; s \}\]</span> 然后寻找最优切分变量<span class="math inline">\(j\)</span>和最优切分点<span class="math inline">\(s\)</span>。具体地，求解 <span class="math display">\[\min_{j,s} [\min_{c_1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2]\]</span> 对固定输入变量<span class="math inline">\(j\)</span>可以找到最优切分点<span class="math inline">\(s\)</span>。 <span class="math display">\[\hat{c}_1 = ave(y_i|x_i \in R_1(j,s)) \quad \hat{c}_2 = ave(y_i|x_i \in R_2(j,s))\]</span> 遍历所有输入变量，找到最优切分变量<span class="math inline">\(j\)</span>，构成一个对<span class="math inline">\((j,s)\)</span>，依次将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样生成的回归树通常称为最小二乘回归树（least squares regression tree）。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-24%20上午10.47.13.png">

</div>
<h4 id="分类树">6.1.2. 分类树</h4>
<p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-24%20上午10.50.00.png">

</div>
<h3 id="cart剪枝">6.2. CART剪枝</h3>
<p>CART剪枝算法从“完全生长”的决策树的底端减去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成：</p>
<ul>
<li>从生成算法产生的决策树<span class="math inline">\(T_0\)</span>底端开始剪枝，直到<span class="math inline">\(T_0\)</span>的根节点，形成一个子树序列<span class="math inline">\(\{T_0,T_1,\cdots,T_n\}\)</span>；</li>
<li>通过交叉验证法在独立的验证集上对子序列进行测试，从中选择最优子树。</li>
</ul>
<p>（1）剪枝，形成一个子树序列<br>
在剪枝过程中，计算子树的损失函数： <span class="math display">\[C_\alpha(T) = C(T) + \alpha |T|\]</span> 其中，<span class="math inline">\(T\)</span>为任意子树，<span class="math inline">\(C(T)\)</span>为对训练数据的预测误差（如基尼指数），<span class="math inline">\(|T|\)</span>为子树的叶节点个数，<span class="math inline">\(\alpha \geq 0\)</span>为参数，<span class="math inline">\(C_\alpha (T)\)</span>为参数是<span class="math inline">\(\alpha\)</span>的子树<span class="math inline">\(T\)</span>的整体损失，参数<span class="math inline">\(\alpha\)</span>权衡训练数据的拟合程度与模型的复杂度。<br>
对于固定的<span class="math inline">\(\alpha\)</span>，一定存在是损失函数<span class="math inline">\(C_\alpha (T)\)</span>最小的子树，将其表示为<span class="math inline">\(T_\alpha\)</span>。<span class="math inline">\(T_\alpha\)</span>在损失函数<span class="math inline">\(C_\alpha (T)\)</span>最小的意义下是最优的。这样的最优子树是唯一的，当<span class="math inline">\(\alpha\)</span>较大时，最优子树<span class="math inline">\(T_\alpha\)</span>偏小；当<span class="math inline">\(\alpha\)</span>较小时，最优子树<span class="math inline">\(T_\alpha\)</span>偏大。极端情况，当<span class="math inline">\(\alpha = 0\)</span>时，整体树是最优的，当<span class="math inline">\(\alpha \to \infty\)</span>时，根节点组成的单节点树是最优的。<br>
可以用递归的方法对树进行剪枝。将<span class="math inline">\(\alpha\)</span>从小增大，<span class="math inline">\(0 &lt; \alpha_1 &lt; \cdots &lt; \alpha_n &lt; + \infty\)</span>,产生一系列的区间<span class="math inline">\([\alpha_i, \alpha_{i+1}), i=0,1,\cdots, n\)</span>；剪枝得到的子树序列对应着区间$ [<em>i, </em>{i+1}), i=0,1,, n<span class="math inline">\(的最优子树序列\)</span>{T_0,T_1,,T_n}<span class="math inline">\(，序列中的子树是嵌套的。 具体地，从整体树\)</span>T_0<span class="math inline">\(开始剪枝。对\)</span>T_0<span class="math inline">\(的任意内部节点\)</span>t<span class="math inline">\(，以\)</span>t$为单节点树的损失函数是 <span class="math display">\[C_\alpha (t) = C(t) + \alpha\]</span> 以<span class="math inline">\(t\)</span>为根节点的子树<span class="math inline">\(T_t\)</span>的损失函数是 <span class="math display">\[C_\alpha (T_t) = C(T_t) + \alpha |T_i|\]</span> 当<span class="math inline">\(\alpha = 0\)</span>即<span class="math inline">\(\alpha\)</span>充分小时，有不等式 <span class="math display">\[C_\alpha (T_t) &lt; C_\alpha (t)\]</span> 当<span class="math inline">\(\alpha\)</span>增大时，在某一<span class="math inline">\(\alpha\)</span>有 <span class="math display">\[C_\alpha (T_t) = C_\alpha (t)\]</span> 当<span class="math inline">\(\alpha\)</span>再增大时，不等式反向。只要<span class="math inline">\(\alpha = \frac{C(t) - C(T_t)}{|T_t| - 1}\)</span>，<span class="math inline">\(T_t\)</span>与<span class="math inline">\(t\)</span>有相同的损失函数值，而<span class="math inline">\(t\)</span>的节点少，因此<span class="math inline">\(t\)</span>比<span class="math inline">\(T_t\)</span>更可取，对<span class="math inline">\(T_t\)</span>进行剪枝。<br>
为此，对<span class="math inline">\(T_0\)</span>中每一个内部节点<span class="math inline">\(t\)</span>，计算 <span class="math display">\[g(t) = \frac{C(t) - C(T_t)}{|T_t| - 1}\]</span> 上式表示剪枝后整体损失函数减小的程度。在<span class="math inline">\(T_0\)</span>中剪去<span class="math inline">\(g(t)\)</span>最小的<span class="math inline">\(T_t\)</span>，将得到的子树作为<span class="math inline">\(T_1\)</span>，同时将最小的<span class="math inline">\(g(t)\)</span>设为<span class="math inline">\(\alpha_1\)</span>。<span class="math inline">\(T_1\)</span>为区间<span class="math inline">\([\alpha_1,\alpha_2)\)</span>的最优子树。<br>
如此剪枝下去，直至得到根节点。在这一过程中，不断地增加<span class="math inline">\(\alpha\)</span>的值，产生新的区间。<br>
（2）在剪枝得到的子树序列<span class="math inline">\(T_0,T_1,\cdots,T_n\)</span>中通过交叉验证选取最优子树<span class="math inline">\(T_\alpha\)</span> 具体地，利用独立的验证数据集，测试子树序列<span class="math inline">\(T_0,T_1,\cdots,T_n\)</span>中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树<span class="math inline">\(T_0,T_1,\cdots,T_n\)</span>都对应于一个参数<span class="math inline">\(\alpha_1, \alpha_2, \cdots, \alpha_n\)</span>。所以，当最优子树<span class="math inline">\(T_k\)</span>确定时，对应的<span class="math inline">\(\alpha_k\)</span>也确定了，即得到最优决策树<span class="math inline">\(T_\alpha\)</span>。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-25%20下午3.59.08.png">

</div>
<h2 id="多变量决策树">7. 多变量决策树</h2>
<p>如果把每个属性视为坐标空间中的一个坐标轴，则<span class="math inline">\(d\)</span>个属性描述的样本就对应了<span class="math inline">\(d\)</span>维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。决策树所形成的分类边界有一个明显的特点：轴平行（axis-parallel），即它的分类边界由若干个与坐标轴平行的分段组成。这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。但在学习任务的真实分类边界较为复杂时，必须使用很多段划分才能获得较好的近似，决策树也会相当复杂，由于要进行大量的属性测试，预测时间开销也会很大。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-25%20下午4.13.34.png">

</div>
<p>若能使用斜的划分边界，则决策树模型将大为简化。“多变量决策树”（multivariable decision tree）就能实现这样的斜划分甚至更为复杂划分的决策树。<br>
以实现斜划分的多变量决策树为例，在此类决策树中，非叶节点不再是仅对某个属性，而是对属性的线性组合进行测试：每个非叶节点是一个形如<span class="math inline">\(\sum_{i=1}^d w_i a_i = t\)</span>的线性分类器，其中<span class="math inline">\(w_i\)</span>是属性<span class="math inline">\(a_i\)</span>的权重，<span class="math inline">\(w_i\)</span>和<span class="math inline">\(t\)</span>可在该节点所含的样本集合属性集上学得。与传统的“单变量决策树”不同，在多变量决策树的学习过程中，不是为每个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-25%20下午4.19.43.png">

</div>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/决策树/" rel="tag"># 决策树</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/21/机器学习/贝叶斯网/" rel="next" title="贝叶斯网">
                <i class="fa fa-chevron-left"></i> 贝叶斯网
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/03/机器学习/支持向量机/" rel="prev" title="支持向量机">
                支持向量机 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Übersicht
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">silen Zhou</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">27</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    <span class="site-state-item-count">18</span>
                    <span class="site-state-item-name">Tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#决策树"><span class="nav-number">1.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树模型与学习"><span class="nav-number">1.1.</span> <span class="nav-text">1. 决策树模型与学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树模型"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1. 决策树模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树学习"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2. 决策树学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征选择"><span class="nav-number">1.2.</span> <span class="nav-text">2. 特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1. 信息增益</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益比"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2. 信息增益比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基尼指数"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3. 基尼指数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树剪枝"><span class="nav-number">1.3.</span> <span class="nav-text">3. 决策树剪枝</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#连续值与缺失值"><span class="nav-number">1.4.</span> <span class="nav-text">4. 连续值与缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#连续值处理"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1. 连续值处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缺失值处理"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2. 缺失值处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#id3和c4.5算法实现"><span class="nav-number">1.5.</span> <span class="nav-text">5. ID3和C4.5算法实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#id3算法"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1. ID3算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#c4.5算法"><span class="nav-number">1.5.2.</span> <span class="nav-text">5.2. C4.5算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cart算法"><span class="nav-number">1.6.</span> <span class="nav-text">6. CART算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#cart生成"><span class="nav-number">1.6.1.</span> <span class="nav-text">6.1. CART生成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#回归树"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">6.1.1. 回归树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分类树"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">6.1.2. 分类树</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cart剪枝"><span class="nav-number">1.6.2.</span> <span class="nav-text">6.2. CART剪枝</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多变量决策树"><span class="nav-number">1.7.</span> <span class="nav-text">7. 多变量决策树</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">silen Zhou</span>

  

  
</div>




  <div class="powered-by">Erstellt mit  <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.2</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.2"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
