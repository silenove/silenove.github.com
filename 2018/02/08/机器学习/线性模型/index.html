<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-EN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.0.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="机器学习,线性回归," />


<meta name="description" content="线性模型 1. 基本形式 给定由\(d\)个属性描述的示例\(x=(x_1,x_2,\cdots,x_d)\)，其中\(x_i\)是\(x\)在第\(i\)个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即 \[f(\pmb{x})=w_1x_1 + w_2x_2+ \cdots + w_dx_d + b\] 一般用向量形式写成 \[f(\pmb">
<meta name="keywords" content="机器学习,线性回归">
<meta property="og:type" content="article">
<meta property="og:title" content="线性模型">
<meta property="og:url" content="http://yoursite.com/2018/02/08/机器学习/线性模型/index.html">
<meta property="og:site_name" content="silenove blogs">
<meta property="og:description" content="线性模型 1. 基本形式 给定由\(d\)个属性描述的示例\(x=(x_1,x_2,\cdots,x_d)\)，其中\(x_i\)是\(x\)在第\(i\)个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即 \[f(\pmb{x})=w_1x_1 + w_2x_2+ \cdots + w_dx_d + b\] 一般用向量形式写成 \[f(\pmb">
<meta property="og:locale" content="zh-EN">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-08%20下午3.25.28.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-10%20下午10.39.27.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-11%20下午10.43.12.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-11%20下午10.57.53.png">
<meta property="og:updated_time" content="2018-02-12T03:03:30.539Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性模型">
<meta name="twitter:description" content="线性模型 1. 基本形式 给定由\(d\)个属性描述的示例\(x=(x_1,x_2,\cdots,x_d)\)，其中\(x_i\)是\(x\)在第\(i\)个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即 \[f(\pmb{x})=w_1x_1 + w_2x_2+ \cdots + w_dx_d + b\] 一般用向量形式写成 \[f(\pmb">
<meta name="twitter:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-08%20下午3.25.28.png">






  <link rel="canonical" href="http://yoursite.com/2018/02/08/机器学习/线性模型/"/>


  <title>线性模型 | silenove blogs</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-EN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">silenove blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">沿路旅程如歌褪变</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/08/机器学习/线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="silen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="silenove blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">线性模型</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-08T10:44:15+08:00">2018-02-08</time>
            

            
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="线性模型">线性模型</h1>
<h2 id="基本形式">1. 基本形式</h2>
<p>给定由<span class="math inline">\(d\)</span>个属性描述的示例<span class="math inline">\(x=(x_1,x_2,\cdots,x_d)\)</span>，其中<span class="math inline">\(x_i\)</span>是<span class="math inline">\(x\)</span>在第<span class="math inline">\(i\)</span>个属性上的取值，线性模型（linear model）试图学得一个通过属性的线性组合来进行预测的函数，即 <span class="math display">\[f(\pmb{x})=w_1x_1 + w_2x_2+ \cdots + w_dx_d + b\]</span> 一般用向量形式写成 <span class="math display">\[f(\pmb{x})=\pmb{w}^T \pmb{x} + b\]</span> 其中<span class="math inline">\(w=(w_1,w_2,\cdots,w_d)\)</span>，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>学得之后，模型就得以确定。<br>
线性模型形式简单、易于建模，许多功能强大的非线性模型（nonlinear model）可在线性模型的基础上通过引入层级结构或高维映射而得。此外，由于<span class="math inline">\(w\)</span>直观表达了各属性在预测中的重要性，因此线性模型有很好的可解释性（comprehensibility）。</p>
<h2 id="线性回归">2. 线性回归</h2>
<p>线性回归是一种监督学习下的线性模型，试图从给定数据集中学习一个线性模型来尽可能准确地预测实值输出标记。</p>
<h3 id="一元线性回归">2.1. 一元线性回归</h3>
<p>一元线性回归指输入属性的数目只有一个，试图学得： <span class="math display">\[f(x_i) = wx_i + b, \text{使得}f(x_i) \approx y_i.\]</span> 确定公式中的<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>，关键在于如何衡量<span class="math inline">\(f(x)\)</span>和<span class="math inline">\(y\)</span>之间的差别。均方误差是回归任务中最常用的性能度量，因此可以试图让均方误差最小化，即 <span class="math display">\[\begin{align}
    (w^*,b^*) &amp;= \arg \min_{(w,b)} \sum_{i=1}^m(f(x_i) - y_i)^2 \nonumber \\
    &amp;= \arg \min_{(w,b)} \sum_{i=1}^m(y_i - wx_i - b)^2 \nonumber
\end{align}\]</span> 均方误差有非常好的几何意义。它对应了常用的欧氏距离，基于均方误差最小化来进行模型求解的方法称为“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，是所有的样本到直线上的欧氏距离之和最小。<br>
求解<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>就是使<span class="math inline">\(E(w,b)=\sum_{i=1}^m(y_i-wx_i-b)^2\)</span>最小化的过程，称为线性模型的最小二乘“参数估计”，可以将<span class="math inline">\(E(w,b)\)</span>分别对<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>求导，得到 <span class="math display">\[\begin{align}
    \frac{\partial E(w,b)}{\partial w} &amp;= 2(w \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i-b)x_i) \nonumber \\
    \frac{\partial E(w,b)}{\partial b} &amp;= 2(mb - \sum_{i=1}^m (y_i-wx_i)) \nonumber
\end{align}\]</span> 然后令导数为0可得到<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>最优解的闭式解： <span class="math display">\[\begin{align}
    w &amp;= \frac{\sum_{i=1}^m (y_i - \bar{y})x_i}{\sum_{i=1}^m x_i^2 - \frac{1}{m} (\sum_{i=1}^m x_i)^2} \nonumber \\
    b &amp;= \frac{1}{m} \sum_{i=1}^m (y_i - wx_i) \nonumber
\end{align}\]</span> 其中<span class="math inline">\(\bar{x} = \frac{1}{m} \sum_{i=1}^m x_i\)</span>。</p>
<h3 id="多元线性回归">2.2. 多元线性回归</h3>
<p>多元线性回归指样本由多个属性描述，试图学得 <span class="math display">\[f(\pmb{x}_i) = \pmb{w}^T \pmb{x}_i + b, \text{使得}f(\pmb{x}_i) \approx y_i\]</span> 为了便于讨论，将<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>吸收入向量形式<span class="math inline">\(\hat{w} = (w,b)\)</span>中，把数据集<span class="math inline">\(D\)</span>表示为一个<span class="math inline">\(m \times (d+1)\)</span>大小的矩阵<span class="math inline">\(X\)</span>，即 <span class="math display">\[\mathbf{X} = \left [ \begin{matrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} &amp; 1 \\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2d} &amp; 1 \\ \vdots &amp; \vdots &amp; \cdots &amp; \vdots &amp; \vdots \\ x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{md} &amp; 1\end{matrix} \right ] = \left [ \begin{matrix} \pmb{x}_1^T &amp; 1 \\ \pmb{x}_2^T &amp; 1 \\ \vdots &amp; \vdots \\ \pmb{x}_m^T &amp; 1 \end{matrix} \right ]\]</span> 把标记也用向量的形式表示<span class="math inline">\(\pmb{y}= (y_1,y_2,\cdots,y_m)\)</span>，则有： <span class="math display">\[\hat{\pmb{w}}^* = \arg \min_{\hat{\pmb{w}}} (\pmb{y} - \mathbf{X} \hat{\pmb{w}})^T (\pmb{y} - \mathbf{X} \hat{\pmb{w}})\]</span> 令<span class="math inline">\(E_{\hat{\pmb{w}}} = (\pmb{y} - \mathbf{X} \hat{\pmb{w}})^T (\pmb{y} - \mathbf{X} \hat{\pmb{w}})\)</span>，对<span class="math inline">\(\hat{\pmb{w}}\)</span>求导得到 <span class="math display">\[\frac{\partial E_{\hat{\pmb{w}}}}{\partial \hat{\pmb{w}}} = 2 \mathbf{X}^T (\mathbf{X} \hat{\pmb{w}} - \pmb{y})\]</span> 令上式为0求得<span class="math inline">\(\hat{\pmb{w}}\)</span>最优解的闭式解，但由于涉及到矩阵的逆运算，所以要讨论<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>是否满秩： （1）<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>满秩<br>
当<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>为满秩矩阵（full-rank matrix）或正定矩阵（positive definite matrix）时，令导数为0求得： <span class="math display">\[\hat{\pmb{w}}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T y\]</span> 学得多元线性回归模型为 <span class="math display">\[f(\hat{\pmb{x}_i}) = \hat{\pmb{x}}_i^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T y\]</span> （2）<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>不满秩<br>
限时任务中<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>往往不是满秩矩阵，数据的属性描述多于样例数，导致<span class="math inline">\(\mathbf{X}\)</span>的列数多余行数，<span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>显然不满秩（方程组中因变量过多导致解出多组解），解出多个<span class="math inline">\(\hat{\pmb{w}}\)</span>都能使均方误差最小化，选择哪一个作为输出将由学习算法的归纳偏好决定，常见的做法是引入正则化项。</p>
<h2 id="logistic回归">3. logistic回归</h2>
<h3 id="logistic分布">3.1. logistic分布</h3>
<p>设<span class="math inline">\(X\)</span>是连续随机变量，<span class="math inline">\(X\)</span>服从logistic分布是指<span class="math inline">\(X\)</span>具有下列分布函数和密度函数： <span class="math display">\[\begin{align}
    F(x) &amp;= P(X \leq x) = \frac{1}{1 + e^{-(x-\mu)/ \gamma}} \nonumber \\
    f(x) &amp;= F&#39;(x) = \frac{e^{-(x-\mu)/ \gamma}}{\gamma(1+e^{-(x-\mu)/ \gamma})^2} \nonumber
\end{align}\]</span> 上式中，<span class="math inline">\(\mu\)</span>为位置参数，<span class="math inline">\(\gamma &gt; 0\)</span>为形状参数。 <img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-08%20下午3.25.28.png"> 分布函数属于logistic函数，其图形是一条S曲线（sigmoid curve），该曲线一点<span class="math inline">\((\mu, \frac{1}{2})\)</span>为中心对称，即满足 <span class="math display">\[F(-x + \mu) - \frac{1}{2} = -F(x + \mu) + \frac{1}{2}\]</span></p>
<h3 id="二项logistic回归模型">3.2. 二项logistic回归模型</h3>
<p>二项logistic回归模型（binomial logistic regression model）是一种分类模型，形式为参数化的logistic分布，有条件概率分布<span class="math inline">\(P(Y|X)\)</span>表示，随机变量<span class="math inline">\(X\)</span>取值为实数，随机变量<span class="math inline">\(Y\)</span>取值为1或0。通过监督学习的方法估计模型参数。<br>
二项logistic回归模型的条件概率分布： <span class="math display">\[\begin{align}
P(Y=1|\pmb{x}) &amp;= \frac{\exp{(\pmb{w}· \pmb{x} + b)}}{1 + \exp{(\pmb{w}· \pmb{x} + b)}} \nonumber \\
P(Y=0|\pmb{x}) &amp;= \frac{1}{1 + \exp{(\pmb{w}· \pmb{x} + b)}} \nonumber
\end{align}\]</span> logistic回归比较两个条件概率值的大小，将实例<span class="math inline">\(x\)</span>分到概率值较大的那一类。<br>
为了方便，将权值向量和输入向量加以扩充，仍记作<span class="math inline">\(\pmb{w}\)</span>，<span class="math inline">\(\pmb{x}\)</span>，即<span class="math inline">\(\pmb{w} = (w^{(1)},w^{(2)}, \cdots, w^{(n)}, b)^T\)</span>，<span class="math inline">\(\pmb{x} = (x^{(1)},x^{(2)}, \cdots, x^{(n)},1)^T\)</span>，此时logistic回归模型如下： <span class="math display">\[\begin{align}
P(Y=1|\pmb{x}) &amp;= \frac{\exp{(\pmb{w}· \pmb{x})}}{1 + \exp{(\pmb{w}· \pmb{x})}} \nonumber \\
P(Y=0|\pmb{x}) &amp;= \frac{1}{1 + \exp{(\pmb{w}· \pmb{x})}} \nonumber
\end{align}\]</span> 一个事件发生的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值，如果事件发生的概率是<span class="math inline">\(p\)</span>，那么该事件的几率是<span class="math inline">\(\frac{p}{1-p}\)</span>，该事件的对数几率（log odds）或logit函数是 <span class="math display">\[logit(p) = \log \frac{p}{1-p}\]</span> 对logistic回归而言，对数几率为 <span class="math display">\[\log \frac{P(Y=1|x)}{1 - P(Y=1|x)} = \pmb{w} · \pmb{x}\]</span> 可以看出，在logistic回归模型中，输出<span class="math inline">\(Y=1\)</span>的对数几率是输入<span class="math inline">\(x\)</span>的线性函数。或者说，输出<span class="math inline">\(Y=1\)</span>的对数几率是由输入<span class="math inline">\(x\)</span>的线性函数表示的模型，即logistic回归模型。<br>
<strong>logistic回归也称为对数几率回归</strong>。</p>
<h3 id="模型参数估计">3.3. 模型参数估计</h3>
<p>logistic回归模型学习时，对于给定的训练数据集<span class="math inline">\(T=\{(x_1,y_1), (x_2,y_2), \cdots, (x_N,y_N)\}\)</span>，其中，<span class="math inline">\(\pmb{x}_i \in \mathbf{R}^n\)</span>，<span class="math inline">\(y_i \in \{0,1\}\)</span>，应用极大似然估计法估计模型参数，从而得到logistic回归模型。<br>
设<span class="math inline">\(P(Y=1|\pmb{x}) = \pi(\pmb{x})\)</span>，<span class="math inline">\(P(Y=0|\pmb{x}) = 1 - \pi(\pmb{x})\)</span>： 似然函数为 <span class="math display">\[\prod_{i=1}^N [\pi(\pmb{x}_i)]^{y_i}[1-\pi (\pmb{x}_i)]^{1-y_i}\]</span> 对数似然函数为 <span class="math display">\[\begin{align}
    L(\pmb{w}) &amp;= \sum_{i=1}^N [y_i \log{\pi(\pmb{x}_i)} +(1-y_i) \log{(1-\pi(\pmb{x}_i))}] \nonumber \\
    &amp;= \sum_{i=1}^N[y_i \log{\frac{\pi(\pmb{x}_i)}{1 - \pi(\pmb{x}_i)}} + \log{(1 - \pi(\pmb{x}_i))}] \nonumber \\
    &amp;= \sum_{i=1}^N[y_i(\pmb{w}·\pmb{x}_i) - \log{(1 + \exp{(\pmb{w}·\pmb{x}_i)})}] \nonumber
\end{align}\]</span> 对<span class="math inline">\(L(\pmb{w})\)</span>求极大值，得到<span class="math inline">\(\pmb{w}\)</span>的估计值。问题变成了以对数似然函数为目标函数的最优化问题，通常采用的方法是梯度下降法及拟牛顿法。<br>
假设<span class="math inline">\(\pmb{w}\)</span>的极大似然估计是<span class="math inline">\(\hat{\pmb{w}}\)</span>，那么学到的logistic回归模型为 <span class="math display">\[\begin{align}
P(Y=1|\pmb{x}) &amp;= \frac{\exp{(\hat{\pmb{w}}· \pmb{x})}}{1 + \exp{(\hat{\pmb{w}}· \pmb{x})}} \nonumber \\
P(Y=0|\pmb{x}) &amp;= \frac{1}{1 + \exp{(\hat{\pmb{w}}· \pmb{x})}} \nonumber
\end{align}\]</span></p>
<h3 id="多项logistic回归">3.4. 多项logistic回归</h3>
<p>上面介绍的二项logistic回归模型用于二类分类。可以将其推广为多项logistic回归模型（multi-nominal logistic regression model），用于多类分类。假设离散型随机变量<span class="math inline">\(Y\)</span>的取值集合为<span class="math inline">\(\{1,2, \cdots, K\}\)</span>，那么多项logistic回归模型为 <span class="math display">\[\begin{equation}
    P(Y=k|\pmb{x}) = \frac{\exp{(\pmb{w}_k · \pmb{x})}}{1 + \sum_{k=1}^{K-1} \exp{(\pmb{w}_k · \pmb{x})}}, k=1,2,\cdots,K-1 \nonumber \\
    P(Y=K|\pmb{x}) = \frac{1}{1 + \sum_{k=1}^{K-1} \exp{(\pmb{w}_k · \pmb{x})}} \nonumber
\end{equation}\]</span> 式中，<span class="math inline">\(\pmb{x} \in \mathbf{R}^{n+1}\)</span>，<span class="math inline">\(w_k \in \mathbf{R}^{n+1}\)</span>。</p>
<h2 id="线性判别分析">4. 线性判别分析</h2>
<p>线性判别分析（Linear Discriminant Analysis, LDA）是一种经典的线性学习方法。在二分类问题上最早由Fisher提出，也成为“Fisher判别分析”。</p>
<h3 id="二分类lda">4.1. 二分类LDA</h3>
<p>LDA的思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置类确定样本的类别。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-10%20下午10.39.27.png">

</div>
<p>设有数据集<span class="math inline">\(D=\{(x_i,y_i)\}_{i=1}^m\)</span>，<span class="math inline">\(y_i \in \{0,1\}\)</span>,<span class="math inline">\(X_i\)</span>表示第<span class="math inline">\(i \in \{0,1\}\)</span>类示例的集合，<span class="math inline">\(\mu_i\)</span>表示均值向量，<span class="math inline">\(\Sigma_i\)</span>表示协方差矩阵。将数据投影到直线<span class="math inline">\(y=\pmb{w}^T \pmb{x}\)</span>上，两类样本的中心的投影分别为<span class="math inline">\(\pmb{w}^T \pmb{\mu}_0\)</span>和<span class="math inline">\(\pmb{w}^T \pmb{\mu}_1\)</span>，协方差为<span class="math inline">\(\pmb{w}^T \Sigma_0 \pmb{w}\)</span>和<span class="math inline">\(\pmb{w}^T \Sigma_1 \pmb{w}\)</span>，均为实数：<br>
(1)投影后两类样本的中心：<br>
<span class="math display">\[m_j = \frac{1}{|C_j|} \sum_{x_i \in C_j} \pmb{w}^T \pmb{x}_i = \pmb{w}^T(\frac{1}{|C_j|} \sum_{x_i \in C_j} \pmb{x}_i) =  \pmb{w}^T \pmb{\mu}_j\]</span> (2)投影后两类样本的协方差： <span class="math display">\[\begin{align}
S_j^2 = \sum_{i \in C_j} (y_i-m_j)^2 &amp;= \sum_{i \in C_j} (\pmb{w}^T (\pmb{x}_i - \mu_j))^2  \nonumber \\
&amp;= \sum_{i \in C_j} (\pmb{w}^T (\pmb{x}_i - \pmb{\mu_j})) (\pmb{w}^T (\pmb{x}_i - \pmb{\mu_j}))^T \nonumber \\
&amp;= \sum_{i \in C_j} \pmb{w}^T (\pmb{x}_i - \mu_j) (\pmb{x}_i - \mu_j)^T \pmb{w} \nonumber \\
&amp;= \pmb{w}^T (\sum_{i \in C_j}(\pmb{x}_i - \mu_j) (\pmb{x}_i - \mu_j)^T) \pmb{w}
\nonumber \\
&amp;= \pmb{w}^T \Sigma_j \pmb{w} \nonumber
\end{align}\]</span> 欲使同类样例的投影点尽可能接近，异类样例的投影点尽可能远离，则可最大化目标： <span class="math display">\[J = \frac{||\pmb{w}^T \mu_0 - \pmb{w}^T \mu_1||_2^2}{\pmb{w}^T \Sigma_0 \pmb{w} + \pmb{w}^T \Sigma_1 \pmb{w}} = \frac{\pmb{w}^T (\mu_0 - \mu_1) (\mu_0 - \mu_1)^T \pmb{w}}{\pmb{w}^T (\Sigma_0 - \Sigma_1) \pmb{w}}\]</span></p>
<p><strong>类内散度矩阵：</strong> <span class="math display">\[S_w = \Sigma_0 + \Sigma_1 = \sum_{\pmb{x} \in X_0} (\pmb{x} - \pmb{\mu}_0)(\pmb{x} - \pmb{\mu}_0)^T + \sum_{\pmb{x} \in X_1} (\pmb{x} - \pmb{\mu}_1)(\pmb{x} - \pmb{\mu}_1)^T\]</span></p>
<p><strong>类间散度矩阵：</strong> <span class="math display">\[S_b = (\pmb{\mu}_0 - \pmb{\mu}_1) (\pmb{\mu}_0 - \pmb{\mu}_1)^T\]</span></p>
<p>目标函数重写为： <span class="math display">\[J = \frac{\pmb{w}^T S_b \pmb{w}}{\pmb{w}^T S_w \pmb{w}}\]</span> 上式为LDA欲最大化的目标，即<span class="math inline">\(S_b\)</span>与<span class="math inline">\(S_w\)</span>的广义瑞利商（其他博客中有介绍）。其中分子、分母都是关于<span class="math inline">\(\pmb{w}\)</span>的二次项，因此上式的解与<span class="math inline">\(\pmb{w}\)</span>的长度无关，只与其方向有关。不是一般性 ，令<span class="math inline">\(\pmb{w}^T S_w \pmb{w} = 1\)</span>，目标公式为： <span class="math display">\[\begin{equation}
    \min_{\pmb{w}} - \pmb{w}^T S_b \pmb{w} \nonumber \\
    s.t. \quad \pmb{w}^T S_w \pmb{w} = 1 \nonumber
\end{equation}\]</span> Lagrange函数： <span class="math display">\[\begin{equation}
    L(\pmb{w}, \lambda) = - \pmb{w}^T S_b \pmb{w} + \lambda (\pmb{w}^T S_w \pmb{w} - 1) \nonumber \\
    \frac{\partial L(\pmb{w}, \lambda)}{\partial \pmb{w}} = -S_b \pmb{w} + \lambda S_w \pmb{w} = 0 \nonumber \\
    S_b \pmb{w} = \lambda S_w \pmb{w} \nonumber
\end{equation}\]</span> <span class="math inline">\(\lambda\)</span>为拉格朗日乘子，因为<span class="math inline">\(S_b \pmb{w}\)</span>的方向恒为<span class="math inline">\(\mu_0 - \mu_1\)</span>，令 <span class="math display">\[S_b \pmb{w} = \lambda (\pmb{\mu}_0 - \pmb{\mu}_1)\]</span> 带入式中求得： <span class="math display">\[\pmb{w} = S_w^{-1} (\pmb{\mu}_0 - \pmb{\mu}_1)\]</span> 考虑到数值稳定性，实践中通常采用奇异值分解的方法求<span class="math inline">\(S_w^{-1}\)</span>： <span class="math display">\[\begin{equation}
    S_w = U \Sigma V^T \nonumber \\
    S_w^{-1} = V \Sigma^{-1} U^T \nonumber
\end{equation}\]</span> <span class="math inline">\(\Sigma\)</span>是一个是对角矩阵，其对角线上的元素是<span class="math inline">\(S_w\)</span>的奇异值。<br>
<strong>LDA还可以从贝叶斯决策理论的角度阐释，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。</strong></p>
<h3 id="多分类lda">4.2. 多分类LDA</h3>
<p>假设存在<span class="math inline">\(N\)</span>个类，且第<span class="math inline">\(i\)</span>类示例数为<span class="math inline">\(m_i\)</span>，定义<strong>全局散度矩阵</strong>： <span class="math display">\[S_t = S_b + S_w = \sum_{i=1}^m (\pmb{x}_i - \pmb{\mu}) (\pmb{x}_i - \pmb{\mu})^T\]</span> 其中<span class="math inline">\(\mu\)</span>是所有示例的均值向量，将类内散度矩阵<span class="math inline">\(S_w\)</span>重定义为每个类别的类内散度矩阵之和，即： <span class="math display">\[S_w = \sum_{i=1}^N S_{w_i}\]</span> 其中 <span class="math display">\[S_{w_i} = \sum_{\pmb{x} \in X_i} (\pmb{x}_i - \pmb{\mu}_i) (\pmb{x}_i - \pmb{\mu}_i)^T\]</span> 由以上公式推得： <span class="math display">\[S_b = S_t - S_w = \sum_{i=1}^N m_i (\pmb{\mu}_i - \pmb{\mu}) (\pmb{\mu}_i - \pmb{\mu})^T\]</span> 多分类LDA可以有多种实现方法，使用<span class="math inline">\(S_b\)</span>，<span class="math inline">\(S_w\)</span>，<span class="math inline">\(S_t\)</span>中任意两个即可，常见的一种实现是采用优化目标： <span class="math display">\[\max_{\pmb{W}} \frac{tr(\pmb{W}^T S_b \pmb{W})}{tr(\pmb{W}^T S_w \pmb{W})}\]</span> 式中<span class="math inline">\(\pmb{W} \in \pmb{R}^{d \times (N-1)}\)</span>，<span class="math inline">\(tr(·)\)</span>表示矩阵的迹（trace），可通过广义特征值求解： <span class="math display">\[S_b \pmb{W} = \lambda S_w \pmb{W}\]</span> <span class="math inline">\(W\)</span>的闭式解则是<span class="math inline">\(S_w^{-1} S_b\)</span>的<span class="math inline">\(d&#39;\)</span>个最大的非零广义特征值所对应的特征向量组成的矩阵，<span class="math inline">\(d&#39; \leq N-1\)</span>。<br>
若将<span class="math inline">\(W\)</span>是为一个投影矩阵，则多分类LDA将样本投影到<span class="math inline">\(d&#39;\)</span>维空间，<span class="math inline">\(d&#39;\)</span>通常远小于数据原有的属性数<span class="math inline">\(d\)</span>。可以通过这个投影来减小样本点的维数，并且投影过程中使用了类别信息，因此LDA也常被是为一种经典的监督降维方法。</p>
<h2 id="多分类学习">5. 多分类学习</h2>
<p>实践中常遇到多分类学习任务，有些二分类学习方法可直接推广到多分类，但在更多情形下，可以基于一些基本策略，利用二分类学习器来解决多分类问题。<br>
多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解。最经典的拆分策略有三种：一对一（One vs One，OvO）、一对其余（One vs Rest，OvR）和多对多（Many vs Many，MvM）。</p>
<h3 id="一对一">5.1. 一对一</h3>
<p>一对一将数据集<span class="math inline">\(D\)</span>对应的类别<span class="math inline">\(y_i \in \{C_1,C_2, \cdots, C_N\}\)</span>两两配对，产生<span class="math inline">\(N(N-1)/2\)</span>个二分类任务。<br>
例如训练区分<span class="math inline">\(C_i\)</span>和<span class="math inline">\(C_j\)</span>的分类器，该分类器将<span class="math inline">\(D\)</span>中的<span class="math inline">\(C_i\)</span>类样例作为正例，<span class="math inline">\(C_j\)</span>类样例作为反例。<br>
在测试阶段，将新样本同时提交给所有分类器，将得到<span class="math inline">\(N(N-1)/2\)</span>个分类结果，最终结果可通过投票产生。</p>
<h3 id="一对其余">5.2. 一对其余</h3>
<p>一对其余是每次将一个类的样例作为正例、所有其他类的样例作为负例作为反例训练<span class="math inline">\(N\)</span>个分类器，在测试时若只有一个分类器预测为正例，则对应的类别标记为最终结果。如果有多个分类器预测为正例，则需考虑各分类器的置信度。<br>
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-11%20下午10.43.12.png"> 通常，一对一的存储开销和测试时间开销比一对其余大。但在训练时，一对其余的每个分类器均使用全部的训练数据，而一对一只使用两类数据，所以类别很多时，一对一的训练时间开销通常比一对其余小。至于预测性能，取决于具体的数据分布，在多数情形下两者差不多。</p>
<h3 id="多对多">5.3. 多对多</h3>
<p>多对多每次将若干个类作为正类，若干个其他类作为反类，并且正类和反类构造必须有特殊的设计，不能随意选取，最常用的多对多的技术是“纠错输出码”（Error Correcting Output Codes，ECOC）。<br>
ECOC是将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性，工作过程主要分为两步：</p>
<ul>
<li>编码：对<span class="math inline">\(N\)</span>个类别作<span class="math inline">\(M\)</span>次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集；共产生<span class="math inline">\(M\)</span>个训练集，可训练出<span class="math inline">\(M\)</span>个分类器；</li>
<li>解码：<span class="math inline">\(M\)</span>个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终结果。</li>
</ul>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-11%20下午10.57.53.png">

</div>
<h2 id="类别不平衡问题">6. 类别不平衡问题</h2>
<p>前面的分类学习方法都有一个共同的基本假设，即不同类别的训练样本数目相当，通常不同类别的训练样例数目稍有差别影响不大，但是如果差别很大就会对学习过程造成困扰。<br>
类别不平衡（class-imbalance）就是指分类任务中不同类别的训练样例数目差别很大的情况。现实的分类学习任务中经常遇到类别不平衡，例如通过拆分法解决多分类问题。<br>
再用<span class="math inline">\(y=\pmb{w}^T \pmb{x} + b\)</span>对新样本 <span class="math inline">\(\pmb{x}\)</span>进行分类时，实际上再用预测出的<span class="math inline">\(y\)</span>值与一个阈值进行比较，通常<span class="math inline">\(y \geq 0.5\)</span>判为正例，否则为反例。<span class="math inline">\(y\)</span>表达了正例的可能性，几率<span class="math inline">\(\frac{y}{1-y}\)</span>则反映了正例可能性与反例可能性的比值，阈值设为0.5表明分类器认为正、反例可能性相同，即分类器决策规则为 <span class="math display">\[若 \quad \frac{y}{1-y} &gt; 1 \quad 则预测为正例\]</span> 然而，当训练集中正、反例数目不同时，<span class="math inline">\(m^+\)</span>表示正例数目，<span class="math inline">\(m^-\)</span>表示反例数目，观测几率为<span class="math inline">\(\frac{m^+}{m^-}\)</span>，通常假设训练集是真实样本总体的无偏采样，因此观测几率代表了真实几率，所以预测几率大于观测几率应判为正例，即 <span class="math display">\[若 \quad \frac{y}{1-y} &gt; \frac{m^+}{m^-} \quad 则预测为正例\]</span> 引出类别不平衡学习的一个基本策略——<strong>再缩放</strong>： <span class="math display">\[\frac{y&#39;}{1-y&#39;} = \frac{y}{1-y} \times \frac{m^-}{m^+}\]</span> 通过将上式与1比较判别正、反例。再缩放是“代价敏感学习”的基础，其中用<span class="math inline">\(cost^+ / cost^-\)</span>代替<span class="math inline">\(m^-/m^+\)</span>。 实际中“训练集是真实样本总体的无偏采样”这个假设往往不成立，所以我们未必能有效地基于训练集观测几率来推断真实几率，现有技术有三类做法：</p>
<ol style="list-style-type: decimal">
<li><strong>欠采样：</strong>去除训练集中的一些反例使得正、反例数目接近，然后进行学习。但是，如果随机丢弃反例，可能会丢失一些重要信息。代表算法EasyEnsemble，利用集成学习机制，将反例划分为若干个集合供不同学习器使用，对于每个学习器都进行了欠采样，从全局看却不会丢失信息；</li>
<li><strong>过采样：</strong>在训练集中增加一些正例使得正、反例数目接近，再进行学习。过采样法不能简单地对初始正例样本进行重复采样，否则会导致过拟合。代表算法SMOTE，通过对训练集里的正例进行插值来产生额外的正例；</li>
<li><strong>阈值移动：</strong>直接基于原始训练集进行学习，在用训练好的分类器进行预测时，使用“再缩放”的方法进行决策。</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/线性回归/" rel="tag"># 线性回归</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/31/碎碎/MacOS知识点-资源路径/" rel="next" title="MacOS知识点--资源路径">
                <i class="fa fa-chevron-left"></i> MacOS知识点--资源路径
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/08/碎碎/瑞利商与广义瑞利商/" rel="prev" title="瑞利商与广义瑞利商">
                瑞利商与广义瑞利商 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">silen Zhou</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">35</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#线性模型"><span class="nav-number">1.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本形式"><span class="nav-number">1.1.</span> <span class="nav-text">1. 基本形式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归"><span class="nav-number">1.2.</span> <span class="nav-text">2. 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一元线性回归"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1. 一元线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多元线性回归"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2. 多元线性回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic回归"><span class="nav-number">1.3.</span> <span class="nav-text">3. logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#logistic分布"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1. logistic分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二项logistic回归模型"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2. 二项logistic回归模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模型参数估计"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3. 模型参数估计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多项logistic回归"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.4. 多项logistic回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性判别分析"><span class="nav-number">1.4.</span> <span class="nav-text">4. 线性判别分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#二分类lda"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1. 二分类LDA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多分类lda"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2. 多分类LDA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多分类学习"><span class="nav-number">1.5.</span> <span class="nav-text">5. 多分类学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一对一"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1. 一对一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一对其余"><span class="nav-number">1.5.2.</span> <span class="nav-text">5.2. 一对其余</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多对多"><span class="nav-number">1.5.3.</span> <span class="nav-text">5.3. 多对多</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#类别不平衡问题"><span class="nav-number">1.6.</span> <span class="nav-text">6. 类别不平衡问题</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">silen Zhou</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.2</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.2"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
