<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-EN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.0.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="强化学习," />


<meta name="description" content="马尔可夫决策过程 1. 马尔可夫决策过程  Markov Decision Process:MDP； 常用于建模序列化决策过程； 行为不仅获得即时奖励，还能改变状态，从而影响长期奖励； 学习状态到行为的映射——策略（多臂赌博机学习\(q_*(a)\)，MDP学习\(q_*(s,a)\)或\(v_*(s)\)）；  2. 要素 智能体和环境按照离散的时间步进行交互：     形式化记号：  智能体在">
<meta name="keywords" content="强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="马尔可夫决策过程">
<meta property="og:url" content="http://yoursite.com/2018/06/27/机器学习/马尔可夫决策过程/index.html">
<meta property="og:site_name" content="silenove blogs">
<meta property="og:description" content="马尔可夫决策过程 1. 马尔可夫决策过程  Markov Decision Process:MDP； 常用于建模序列化决策过程； 行为不仅获得即时奖励，还能改变状态，从而影响长期奖励； 学习状态到行为的映射——策略（多臂赌博机学习\(q_*(a)\)，MDP学习\(q_*(s,a)\)或\(v_*(s)\)）；  2. 要素 智能体和环境按照离散的时间步进行交互：     形式化记号：  智能体在">
<meta property="og:locale" content="zh-EN">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午3.30.16.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午3.45.42.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午4.48.28.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午7.01.37.png">
<meta property="og:updated_time" content="2018-06-27T11:06:07.594Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="马尔可夫决策过程">
<meta name="twitter:description" content="马尔可夫决策过程 1. 马尔可夫决策过程  Markov Decision Process:MDP； 常用于建模序列化决策过程； 行为不仅获得即时奖励，还能改变状态，从而影响长期奖励； 学习状态到行为的映射——策略（多臂赌博机学习\(q_*(a)\)，MDP学习\(q_*(s,a)\)或\(v_*(s)\)）；  2. 要素 智能体和环境按照离散的时间步进行交互：     形式化记号：  智能体在">
<meta name="twitter:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午3.30.16.png">






  <link rel="canonical" href="http://yoursite.com/2018/06/27/机器学习/马尔可夫决策过程/"/>


  <title>马尔可夫决策过程 | silenove blogs</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-EN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">silenove blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">沿路旅程如歌褪变</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/27/机器学习/马尔可夫决策过程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="silen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="silenove blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">马尔可夫决策过程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-27T19:05:39+08:00">2018-06-27</time>
            

            
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="马尔可夫决策过程">马尔可夫决策过程</h1>
<h2 id="马尔可夫决策过程-1">1. 马尔可夫决策过程</h2>
<ul>
<li>Markov Decision Process:MDP；</li>
<li>常用于建模序列化决策过程；</li>
<li>行为不仅获得即时奖励，还能改变状态，从而影响长期奖励；</li>
<li>学习状态到行为的映射——策略（多臂赌博机学习<span class="math inline">\(q_*(a)\)</span>，MDP学习<span class="math inline">\(q_*(s,a)\)</span>或<span class="math inline">\(v_*(s)\)</span>）；</li>
</ul>
<h2 id="要素">2. 要素</h2>
<p><strong>智能体和环境按照离散的时间步进行交互：</strong></p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午3.30.16.png">

</div>
<p><strong>形式化记号</strong>：</p>
<ul>
<li>智能体在时间步<span class="math inline">\(t\)</span>所处的状态记为<span class="math inline">\(S_t \in S\)</span>；</li>
<li>智能体在时间步<span class="math inline">\(t\)</span>所采取的行为记为<span class="math inline">\(A_t \in A(s)\)</span>；</li>
<li>采取行为<span class="math inline">\(A_t\)</span>后智能体转到状态<span class="math inline">\(S_{t+1}\)</span>，并获得奖励<span class="math inline">\(R_{t+1} \in R\)</span>；</li>
<li>马尔科夫决策过程产生的序列记为 <span class="math display">\[S_0, A_0, R_1, S_1, A_1,R_2, \cdots\]</span></li>
</ul>
<h2 id="建模">3. 建模</h2>
<p><strong>模型</strong>：</p>
<p><span class="math display">\[p(s&#39;, r|s, a) \doteq Pr\{S_t=s&#39;, R_t=r|S_{t-1}=s,A_{t-1}=a\}\]</span></p>
<p>这里<span class="math inline">\(p: S \times R \times S \times A \to [0,1]\)</span>满足 <span class="math display">\[\sum_{s&#39; \in S} \sum_{r \in R} p(s&#39;,r|s,a)=1, \quad \text{for all} \quad s \in S, \quad a \in A(s)\]</span></p>
<p><strong>常见推导</strong>：</p>
<p><strong>（1）状态转移概率</strong>： <span class="math display">\[p(s&#39;|s,a) \doteq Pr\{S_t=s&#39;|S_{t-1}=s,A_{t-1}=a\} = \sum_{r \in R} p(s&#39;,r|s,a)\]</span></p>
<p><strong>(2) “状态-行为”对的期望奖励</strong>： <span class="math display">\[r(s,a) \doteq E[R_t|S_{t-1}=s,A_{t-1}=a] = \sum_{r \in R} r \sum_{s&#39; \in S} p(s&#39;,r|s,a)\]</span></p>
<p><strong>(3) &quot;状态-行为-下一状态&quot;的奖励</strong>： <span class="math display">\[r(s,a,s&#39;) \doteq E[R_t|S_{t-1} = s, A_{t-1}=a, S_t = s&#39;] = \sum_{r \in R} r \frac{p(s&#39;,r|s,a)}{p(s&#39;|s,a)}\]</span></p>
<p><strong>示例</strong>： <img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午3.45.42.png"></p>
<h2 id="奖励">4. 奖励</h2>
<h3 id="奖励假设">4.1. 奖励假设</h3>
<p>目标和奖励：</p>
<ul>
<li>目标：长期或最终的；</li>
<li>奖励：即时奖励；</li>
</ul>
<p><strong>奖励假设（reward hypothesis）</strong>：That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of received scalar signal(called reward).</p>
<p>设置奖励：<strong>奖励是我们与智能体进行交互的途径</strong>。</p>
<h3 id="奖励设置">4.2. 奖励设置</h3>
<p><strong>设置奖励是希望智能体能达到期望的目标</strong>：</p>
<ul>
<li>围棋：奖励需要能够实现赢棋这一目标（吃子多少？占领棋盘中心？）；</li>
<li>迷宫：目标是尽快走出迷宫，可设置每走一步，奖励-1；</li>
<li>垃圾回收机器人：目标是在尽可能少的人工干预的情况下回收尽可能多的垃圾，奖励可设置为回收一个垃圾奖励+1，人工干预一次奖励-3。</li>
</ul>
<h3 id="累计奖励">4.3. 累计奖励</h3>
<p><strong>累计奖励（回报return）</strong>： <span class="math display">\[G_t \doteq R_{t+1} + R_{t+2} + \cdots + R_T\]</span> 其中<span class="math inline">\(T\)</span>表示最后一步，对应的状态被称为终止态。</p>
<ul>
<li>具有终止态的马尔可夫决策过程被称为“多幕式”任务；</li>
<li>没有终止态的任务称为连续式任务，其累计奖励记为 <span class="math display">\[G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]</span> 其中<span class="math inline">\(0 \leqslant \gamma \leqslant 1\)</span>表示折扣率。</li>
</ul>
<h3 id="累计奖励的递推公式">4.4. 累计奖励的递推公式</h3>
<p><strong>递推公式</strong>： <span class="math display">\[\begin{align}
    G_t &amp; \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \nonumber \\
    &amp;= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \cdots) \nonumber \\
    &amp;= R_{t+1} + \gamma G_{t+1} \nonumber
\end{align}\]</span></p>
<p><strong>求和公式</strong>： <span class="math display">\[G_t \doteq \sum_{k=t+1}^T \gamma^{k-t-1} R_k\]</span></p>
<p><strong>注意</strong>：<span class="math inline">\(\gamma=1\)</span>和<span class="math inline">\(T=\infty\)</span>不能同时出现。</p>
<h2 id="策略和状态估值函数">5. 策略和状态估值函数</h2>
<p><strong>策略</strong>：</p>
<ul>
<li>状态到行为的映射；</li>
<li>随机式策略：<span class="math inline">\(\pi(a|s)\)</span>；</li>
<li>确定式策略：<span class="math inline">\(a = \pi(s)\)</span>；</li>
</ul>
<p>给定策略<span class="math inline">\(\pi\)</span>，状态估值函数（state-value function）定义为 <span class="math display">\[v_{\pi}(s) \doteq E_{\pi}[G_t|S_t=s] = E_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t=s \right], \quad \text{for all} \quad s \in S\]</span></p>
<p>给定策略<span class="math inline">\(\pi\)</span>，行为估值函数（action-value function）定义为 <span class="math display">\[q_{\pi}(s,a) \doteq E_{\pi} [G_t | S_t=s,A_t=a] = E_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg|S_t=s,A_t=a \right]\]</span></p>
<h2 id="贝尔曼方程">6. 贝尔曼方程</h2>
<p><strong>状态估值函数的贝尔曼方程</strong>： <span class="math display">\[\begin{align}
    v_{\pi}(s) &amp; \doteq E_{\pi} [G_t|S_t=s] \nonumber \\
    &amp;= E_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t=s] \nonumber \\
    &amp;= \sum_a \pi(a|s) \sum_{s&#39;} \sum_r p(s&#39;,r|s,a) \Big[ r+\gamma E_{\pi}[G_{t+1}|S_{t+1}=s&#39;] \Big] \nonumber \\
    &amp;= \sum_a \pi(a|s) \sum_{s&#39;,r} p(s&#39;,r|s,a) \Big[ r + \gamma v_{\pi}(s&#39;) \Big], \quad \text{for all} \quad s \in S \nonumber
\end{align}\]</span></p>
<p><strong>Tip</strong>:Bellman equation expresses a relationship between the value of a state and the values of its successor states.</p>
<p>贝尔曼方程的作用——<strong>贝尔曼方程定义了状态估值函数的依赖关系</strong>：</p>
<ul>
<li>给定策略下，每个状态的估值视为一个变量；</li>
<li>所有状态（假如有<span class="math inline">\(n\)</span>个）的估值根据贝尔曼方程形成一个具有<span class="math inline">\(n\)</span>个方程和<span class="math inline">\(n\)</span>个变量的线性方程组；</li>
<li>求解该方程组即可得到该策略下每个状态的估值。</li>
</ul>
<p><strong>示例</strong>：Gridworld</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午4.48.28.png">

</div>
<h2 id="最优策略">7. 最优策略</h2>
<p>状态估值函数定义了策略中间上的一个偏序：<strong>给定两个策略<span class="math inline">\(\pi\)</span>和<span class="math inline">\(\pi&#39;\)</span>，如果对于所有状态<span class="math inline">\(s\)</span>，都有<span class="math inline">\(v_{\pi}(s) \geqslant v_{\pi&#39;}(s)\)</span>，则可以说<span class="math inline">\(\pi \geqslant \pi&#39;\)</span></strong>。</p>
<p><strong>最优策略</strong>：</p>
<ul>
<li>最优策略<span class="math inline">\(\pi_*\)</span>对应于如下状态估值函数： <span class="math display">\[v_*(s) \doteq \max_{\pi} v_{\pi} (s)\]</span></li>
<li>最优策略可以有多个，所对应的状态估值函数都一样；</li>
<li>最优策略对应的行为估值函数： <span class="math display">\[q_*(s,a) \doteq \max_{\pi} q_{\pi} (s,a)\]</span></li>
</ul>
<h3 id="贝尔曼最优性方程">7.1. 贝尔曼最优性方程</h3>
<p><strong>1. 状态估值函数的贝尔曼最优性方程</strong>： <span class="math display">\[\begin{align}
    v_*(s) &amp;= \max_{a \in A(s)} q_{\pi_*}(s,a) \nonumber \\
    &amp;= \max_a E_{\pi_*} [G_t | S_t=s, A_t=a] \nonumber \\
    &amp;= \max_a E_{\pi_*} [R_{t+1} + \gamma G_{t+1} | S_t = s, A_t=a] \nonumber \\
    &amp;= \max_a E[R_{t+1} + \gamma v_*(S_{t+1})|S_t=s,A_t=a] \nonumber \\
    &amp;= \max_a \sum_{s&#39;,r} p(s&#39;,r|s,a) [r + \gamma v_*(s&#39;)] \nonumber 
\end{align}\]</span></p>
<p><strong>2. 行为估值函数的贝尔曼最优性方程</strong>： <span class="math display">\[\begin{align}
    q_*(s,a) &amp;= E \bigg[ R_{t+1} + \gamma \max_{a&#39;} q_*(S_{t+1},a&#39;) \Big| S_t=s,A_t=a \bigg] \nonumber \\
    &amp;= \sum_{s&#39;,r} p(s&#39;,r|s,a) [r+\gamma v_*(s&#39;)] \nonumber
\end{align}\]</span></p>
<h3 id="寻找最优策略">7.2. 寻找最优策略</h3>
<p><strong>基于状态估值函数的贝尔曼最优性方程</strong>：</p>
<ol style="list-style-type: decimal">
<li>第一步：求解状态估值函数的贝尔曼最优性方程得到最优策略对应的状态估值函数；</li>
<li>第二步：根据状态估值函数的贝尔曼最优性方程，进行一步搜索找到每个状态下的最优性行为：
<ul>
<li>注意：最优策略可以存在多个；</li>
<li>贝尔曼最优性方程的优势，可以采用贪心局部搜索即可得到全局最优解。</li>
</ul></li>
</ol>
<p><strong>基于行为估值函数的贝尔曼最优性方程</strong>：</p>
<ul>
<li>直接得到最优策略。</li>
</ul>
<p><strong>示例</strong>：Gridworld</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-06-27%20下午7.01.37.png">

</div>
<h2 id="寻找最优策略小结">7.3. 寻找最优策略小结</h2>
<p><strong>求解贝尔曼最优性方程寻找最优策略的局限性</strong>：</p>
<ul>
<li>需要知道环境模型<span class="math inline">\(p(s&#39;,r|s,a)\)</span>；</li>
<li>需要高昂的计算代价和内存（存放估值函数）；</li>
<li>依赖于马尔可夫性。</li>
</ul>
<p><strong>实际应用中寻找最优策略的方法</strong>：</p>
<ul>
<li>动态规划方法</li>
<li>蒙特卡洛方法</li>
<li>时序差分方法</li>
<li>参数化方法</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/26/机器学习/多臂赌博机/" rel="next" title="多臂赌博机">
                <i class="fa fa-chevron-left"></i> 多臂赌博机
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">silen Zhou</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">46</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#马尔可夫决策过程"><span class="nav-number">1.</span> <span class="nav-text">马尔可夫决策过程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#马尔可夫决策过程-1"><span class="nav-number">1.1.</span> <span class="nav-text">1. 马尔可夫决策过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#要素"><span class="nav-number">1.2.</span> <span class="nav-text">2. 要素</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#建模"><span class="nav-number">1.3.</span> <span class="nav-text">3. 建模</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#奖励"><span class="nav-number">1.4.</span> <span class="nav-text">4. 奖励</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#奖励假设"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1. 奖励假设</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#奖励设置"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2. 奖励设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#累计奖励"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3. 累计奖励</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#累计奖励的递推公式"><span class="nav-number">1.4.4.</span> <span class="nav-text">4.4. 累计奖励的递推公式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#策略和状态估值函数"><span class="nav-number">1.5.</span> <span class="nav-text">5. 策略和状态估值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#贝尔曼方程"><span class="nav-number">1.6.</span> <span class="nav-text">6. 贝尔曼方程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最优策略"><span class="nav-number">1.7.</span> <span class="nav-text">7. 最优策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#贝尔曼最优性方程"><span class="nav-number">1.7.1.</span> <span class="nav-text">7.1. 贝尔曼最优性方程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#寻找最优策略"><span class="nav-number">1.7.2.</span> <span class="nav-text">7.2. 寻找最优策略</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#寻找最优策略小结"><span class="nav-number">1.8.</span> <span class="nav-text">7.3. 寻找最优策略小结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">silen Zhou</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.2</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.2"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
