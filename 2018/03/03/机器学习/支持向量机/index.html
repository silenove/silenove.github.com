<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-EN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.0.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="机器学习," />


<meta name="description" content="支持向量机 支持向量机（support vector machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机">
<meta property="og:url" content="http://yoursite.com/2018/03/03/机器学习/支持向量机/index.html">
<meta property="og:site_name" content="silenove blogs">
<meta property="og:description" content="支持向量机 支持向量机（support vector machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小">
<meta property="og:locale" content="zh-EN">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午3.51.31.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午3.59.00.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午4.32.38.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午10.26.36.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20上午10.32.24.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20上午10.57.55.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20下午10.05.11.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20下午10.57.26.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-01%20上午10.36.25.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-01%20下午4.25.04.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-01%20下午4.29.22.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-03%20下午10.21.05.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-03%20下午10.38.51.png">
<meta property="og:updated_time" content="2018-03-04T02:46:13.051Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="支持向量机">
<meta name="twitter:description" content="支持向量机 支持向量机（support vector machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小">
<meta name="twitter:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午3.51.31.png">






  <link rel="canonical" href="http://yoursite.com/2018/03/03/机器学习/支持向量机/"/>


  <title>支持向量机 | silenove blogs</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-EN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">silenove blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">沿路旅程如歌褪变</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/03/机器学习/支持向量机/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="silen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="silenove blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">支持向量机</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-03T23:35:07+08:00">2018-03-03</time>
            

            
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="支持向量机">支持向量机</h1>
<p>支持向量机（support vector machines, SVM）是一种二分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。<br>
支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机（linear support vector machine in linearly separable case）、线性支持向量机（linear support vector machine）及费线性支持向量机（non-linear support vector machine）。简单模型是复杂模型的基础，也是复杂模型的特殊情况。<br>
当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时，核函数（kernel function）表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机，这样的方法称为核技巧。</p>
<h2 id="线性可分支持向量机与硬间隔最大化">1. 线性可分支持向量机与硬间隔最大化</h2>
<h3 id="线性可分支持向量机">1.1. 线性可分支持向量机</h3>
<p>假设输入空间与特征空间为两个不同的空间，输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。这样，输入都是由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。<br>
假设给定一个特征空间上的训练数据集 <span class="math display">\[T = \{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots, (\pmb{x}_N,y_N)\}\]</span> 其中，<span class="math inline">\(\pmb{x}_i \in \chi = \mathbf{R}^n\)</span>，<span class="math inline">\(y_i \in \{+1,-1\}, \quad i=1,2,\cdots,N\)</span>，<span class="math inline">\(\pmb{x}_i\)</span>为第<span class="math inline">\(i\)</span>个特征向量，也成为实例，<span class="math inline">\(y_i\)</span>为<span class="math inline">\(\pmb{x}_i\)</span>的类标记，<span class="math inline">\(+1\)</span>表示正例，<span class="math inline">\(-1\)</span>表示负例。假设训练数据集是线性可分的。<br>
学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程<span class="math inline">\(\pmb{w} · \pmb{x} + b = 0\)</span>，它由法向量<span class="math inline">\(\pmb{w}\)</span>和截距<span class="math inline">\(b\)</span>决定，可用<span class="math inline">\((\pmb{w},b)\)</span>表示。分离超平面将特征空间划分为两部分，法向量指向的一侧为正类，另一侧为负类。<br>
当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，此时的解有无穷多个。线性可分支持向量机利用间隔最大化求解最优分离超平面，此时的解是唯一的。<br>
<strong>线性可分支持向量机</strong>：给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为 <span class="math display">\[\pmb{w}^* · \pmb{x} + b^* = 0\]</span> 以及相应的分类决策函数 <span class="math display">\[f(x) = sign(\pmb{w}^* · \pmb{x} + b^*)\]</span> 称为线性可分支持向量机。</p>
<h3 id="函数间隔和几何间隔">1.2. 函数间隔和几何间隔</h3>
<p>一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面<span class="math inline">\(\pmb{w} · \pmb{x} + b = 0\)</span>确定的情况下，<span class="math inline">\(|\pmb{w} · \pmb{x} + b|\)</span>能够相对地表示点<span class="math inline">\(\pmb{x}\)</span>距离超平面的远近。而<span class="math inline">\(\pmb{w} · \pmb{x} + b\)</span>的符号与类标记<span class="math inline">\(y\)</span>的符号是否一致能够表示分类是否正确。所以可用量<span class="math inline">\(y(\pmb{w} · \pmb{x} + b)\)</span>来表示分类的正确性及确信度，这就是函数间隔（functional margin）的概念。<br>
<strong>函数间隔</strong>：对于给定的训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((\pmb{w},b)\)</span>，定义超平面<span class="math inline">\((\pmb{w},b)\)</span>关于样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>的函数间隔为 <span class="math display">\[\hat{\gamma}_i = y_i(\pmb{w} · \pmb{x} + b)\]</span> 定义超平面<span class="math inline">\((\pmb{w},b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的函数间隔为超平面<span class="math inline">\((\pmb{w},b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>的函数间隔的最小值，即 <span class="math display">\[\hat{\gamma} = \min_{i=1,2,\cdots,N} \hat{\gamma}_i\]</span> 函数间隔可以表示分类预测的正确性及确信度。但是在选择分离超平面时，只有函数间隔还不够，因为成比例的改变<span class="math inline">\(\pmb{w}\)</span>和<span class="math inline">\(b\)</span>，例如改为<span class="math inline">\(2\pmb{w}\)</span>和<span class="math inline">\(2b\)</span>，超平面并没有改变，但函数间隔却变为原来的2倍。所以可以对分离超平面的法向量<span class="math inline">\(\pmb{w}\)</span>增加某些约束，如规范化，<span class="math inline">\(||\pmb{w}|| = 1\)</span>，使得间隔是确定的，这时函数间隔称为几何间隔（geometric margin）。<br>
<strong>几何间隔</strong>：对于给定的训练数据集<span class="math inline">\(T\)</span>和超平面<span class="math inline">\((\pmb{w},b)\)</span>，定义超平面<span class="math inline">\((\pmb{w},b)\)</span>关于样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>的几何间隔为 <span class="math display">\[\gamma_i = y_i(\frac{\pmb{w}}{||\pmb{w}||}·\pmb{x}_i + \frac{b}{||\pmb{w}||})\]</span> 定义超平面<span class="math inline">\((\pmb{w},b)\)</span>关于训练数据集<span class="math inline">\(T\)</span>的几何间隔为超平面<span class="math inline">\((\pmb{w},b)\)</span>关于<span class="math inline">\(T\)</span>中所有样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>的几何间隔的最小值，即 <span class="math display">\[\gamma = \min_{i=1,2,\cdots,N} \gamma_i\]</span> 超平面<span class="math inline">\((\pmb{w},b)\)</span>关于样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>的几何间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是实例点到超平面的距离。<br>
从函数间隔和几何间隔的定义可知，二者有如下关系： <span class="math display">\[\gamma_i = \frac{\hat{\gamma}_i}{||\pmb{w}||}, \quad \gamma = \frac{\hat{\gamma}}{||\pmb{w}||}\]</span> 如果<span class="math inline">\(||\pmb{w}||=1\)</span>，那么函数间隔和几何间隔相等。如果超平面参数<span class="math inline">\(\pmb{w}\)</span>和<span class="math inline">\(b\)</span>成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。</p>
<h3 id="间隔最大化">1.3. 间隔最大化</h3>
<p>支持向量机学习的基本想法：求解能够正确划分训练数据集并且几何间隔最大的分离超平面。<br>
对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化。<br>
间隔最大化的直观解释：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。这样的超平面应该对未知的新实例有很好的分类预测能力。</p>
<h4 id="最大间隔分离超平面">1.3.1. 最大间隔分离超平面</h4>
<p>为了求解几何间隔最大的分离超平面，可以表示为下面的约束最优化问题： <span class="math display">\[\begin{align}
    &amp; \max_{\pmb{w},b} \quad \gamma \nonumber \\
    &amp; s.t. \quad y_i(\frac{\pmb{w}}{||\pmb{w}||} · \pmb{x}_i + \frac{b}{||\pmb{w}||}) \geq \gamma, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 即最大化超平面<span class="math inline">\((\pmb{w},b)\)</span>关于训练数据集的几何间隔<span class="math inline">\(\gamma\)</span>，约束条件表示超平面<span class="math inline">\((\pmb{w},b)\)</span>关于每个训练样本点的几何间隔至少是<span class="math inline">\(\gamma\)</span>。<br>
考虑到几何间隔和函数间隔的关系，将上式改写为： <span class="math display">\[\begin{align}
    &amp; \max_{\pmb{w},b} \quad \frac{\hat{\gamma}}{||\pmb{w}||} \nonumber \\
    &amp; s.t. \quad y_i(\pmb{w} · \pmb{x}_i + b) \geq \hat{\gamma}, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 函数间隔的改变并不会对上面最优化问题的不等式约束产生影响，对目标函数的优化也没有影响，即它产生了一个等价的最优化问题。这时可以令<span class="math inline">\(\hat{\gamma} = 1\)</span>，并将其带入到上面的最优化问题，并且最大化<span class="math inline">\(\frac{1}{||\pmb{w}||}\)</span>和最小化<span class="math inline">\(\frac{1}{2} ||\pmb{w}||^2\)</span>是等价的，于是就得到了下面的线性可分支持向量机学习的最优化问题 <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{w},b} \quad \frac{1}{2} ||\pmb{w}||^2 \nonumber \\
    &amp; s.t. \quad y_i(\pmb{w}·\pmb{x}_i + b) - 1 \geq 0, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 这是一个凸二次规划问题。<br>
<strong>凸二次规划问题</strong>是指约束最优化问题 <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{w}} \quad f(\pmb{w}) \nonumber \\
    &amp; s.t. \quad g_i(\pmb{w}) \leq 0, \quad i=1,2,\cdots,N \nonumber \\
    &amp; \qquad \quad h_i(\pmb{w}) = 0, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 其中，目标函数<span class="math inline">\(f(\pmb{w})\)</span>和约束函数<span class="math inline">\(g_i(\pmb{w})\)</span>都是<span class="math inline">\(\mathbf{R}^n\)</span>上的连续可微的凸函数，约束函数<span class="math inline">\(h_i(\pmb{w})\)</span>是<span class="math inline">\(\mathbf{R}^n\)</span>上的仿射函数。<br>
当目标函数<span class="math inline">\(f(\pmb{w})\)</span>是二次函数且约束函数<span class="math inline">\(g_i(\pmb{w})\)</span>是仿射函数时，上述凸最优化问题成为凸二次规划问题。<br>
求解出上述最优化问题的解<span class="math inline">\(\pmb{w}^*\)</span>和<span class="math inline">\(b^*\)</span>，就可以得到最大间隔分离超平面<span class="math inline">\(\pmb{w}^*·\pmb{x} + b^* = 0\)</span>及分类决策函数<span class="math inline">\(f(\pmb{x}) = sign(\pmb{w}^*·\pmb{x} + b^*)\)</span>，即线性可分支持向量机模型。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午3.51.31.png">

</div>
<h4 id="最大间隔分离超平面的存在唯一性">1.3.2. 最大间隔分离超平面的存在唯一性</h4>
<p><strong>若训练数据集<span class="math inline">\(T\)</span>线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</strong></p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午3.59.00.png">

</div>
<h4 id="支持向量和间隔边界">1.3.3. 支持向量和间隔边界</h4>
<p><strong>支持向量（support vector）</strong>：在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束条件式等号成立的点，即 <span class="math display">\[y_i(\pmb{w}·\pmb{x}_i + b) - 1 = 0\]</span> 对于<span class="math inline">\(y_i=1\)</span>的正例点，支持向量所在的超平面： <span class="math display">\[H_1: \pmb{w}·\pmb{x} + b = 1\]</span> 对于<span class="math inline">\(y_i=-1\)</span>的负例点，支持向量所在的超平面： <span class="math display">\[H_2: \pmb{w}·\pmb{x} + b = -1\]</span> 如下图所示，在<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span>上的点就是支持向量：</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午4.32.38.png">

</div>
<p><span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span>平行，并且没有实例点落在它们中间。在<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span>之间形成一条长带，分离超平面与它们平行且位于它们中央。常带的宽度，即<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span>之间的距离成为间隔（margin）。间隔依赖于分离超平面的法向量<span class="math inline">\(\pmb{w}\)</span>，等于<span class="math inline">\(\frac{2}{||\pmb{w}||}\)</span>。<span class="math inline">\(H_1\)</span>和<span class="math inline">\(H_2\)</span>成为间隔边界。<br>
在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。支持向量的个数一般很少，所以支持向量机由很少的“重要”训练样本确定。</p>
<h3 id="学习的对偶算法">1.4. 学习的对偶算法</h3>
<p>为了求解线性可分支持向量机的最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题（dual problem）得到原始问题（primal problem）的最优解，这就是线性可分支持向量机的对偶算法（dual algorithm）。<br>
优点：</p>
<ul>
<li>对偶问题往往更容易求解；</li>
<li>自然引入核函数，进而推广到非线性分类问题。</li>
</ul>
<p>首先构建拉格朗日函数，对每一个不等式引进拉格朗日乘子<span class="math inline">\(\alpha_i \geq 0,\quad i=1,2,\cdots,N\)</span>，定义拉格朗日函数： <span class="math display">\[L(\pmb{w},b,\pmb{\alpha}) = \frac{1}{2} ||\pmb{w}||^2 - \sum_{i=1}^N \alpha_i [y_i (\pmb{w}·\pmb{x} + b)-1]\]</span> 其中<span class="math inline">\(\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_N)^T\)</span>为拉格朗日乘子向量。<br>
根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题： <span class="math display">\[\max_{\pmb{\alpha}} \min_{\pmb{w},b} L(\pmb{w},b,\pmb{\alpha})\]</span> 所以，为了得到对偶问题的解，需要先求<span class="math inline">\(L(\pmb{w},b,\pmb{\alpha})\)</span>对<span class="math inline">\(\pmb{w}\)</span>和<span class="math inline">\(b\)</span>的极小，再求对<span class="math inline">\(\alpha\)</span>的极大。</p>
<p>（1）求<span class="math inline">\(\min_{\pmb{w},b} L(\pmb{w},b,\pmb{\alpha})\)</span><br>
将拉格朗日函数<span class="math inline">\(L(\pmb{w},b,\pmb{\alpha})\)</span>分别对<span class="math inline">\(\pmb{w}\)</span>和<span class="math inline">\(b\)</span>求偏导数并令其等于0： <span class="math display">\[\begin{align}
    \nabla_{\pmb{w}} L(\pmb{w},b,\pmb{\alpha}) &amp;= \pmb{w} - \sum_{i=1}^N \alpha_i y_i x_i = 0 \nonumber \\
    \nabla_b L(\pmb{w},b,\pmb{\alpha}) &amp;= - \sum_{i=1}^N \alpha_i y_i = 0 \nonumber
\end{align}\]</span> 求得 <span class="math display">\[\begin{equation}
    \pmb{w} = \sum_{i=1}^N \alpha_i y_i x_i \nonumber \\
    \sum_{i=1}^N \alpha_i y_i = 0 \nonumber
\end{equation}\]</span> 将求得上式带入朗格朗日函数，得到 <span class="math display">\[\begin{align}
    L(\pmb{w},b,\pmb{\alpha}) &amp;= \frac{1}{2} \sum_{i=1}^N \sum_{i=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i·\pmb{x}_j) - \sum_{i=1}^N \alpha_i y_i ((\sum_{j=1}^N \alpha_j y_j \pmb{x}_j)·\pmb{x}_i + b) + \sum_{i=1}^N \alpha_i \nonumber \\
    &amp;= - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) + \sum_{i=1}^N \alpha_i \nonumber 
\end{align}\]</span> 即 <span class="math display">\[\min_{\pmb{w},b} L(\pmb{w},b,\pmb{\alpha}) = - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) + \sum_{i=1}^N \alpha_i\]</span></p>
<p>（2）求<span class="math inline">\(\min_{\pmb{w},b} L(\pmb{w},b,\pmb{\alpha})\)</span>对<span class="math inline">\(\alpha\)</span>的极大，既是对偶问题 <span class="math display">\[\begin{align}
    &amp; \max_{\pmb{\alpha}} \quad - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) + \sum_{i=1}^N \alpha_i \nonumber \\
    &amp; s.t. \quad \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    &amp; \qquad \quad \alpha_i \geq 0, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 将目标函数由求极大转换成求极小，就得到下面与之等价的对偶最优化问题： <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{\alpha}} \quad \frac{1}{2} \sum_{i=1}^N \sum_{i=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) - \sum_{i=1}^N \alpha_i \nonumber \\
    &amp; s.t. \quad \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    &amp; \qquad \quad \alpha_i \geq 0,\quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 对线性可分训练数据集，假设对偶最优化问题对<span class="math inline">\(\pmb{\alpha}\)</span>的解为<span class="math inline">\(\pmb{\alpha}^* = (\alpha_1^*,\alpha_2^*,\cdots,\alpha_N^*)^T\)</span>，可以由<span class="math inline">\(\pmb{\alpha}^*\)</span>求得原始最优化问题对<span class="math inline">\((\pmb{w},b)\)</span>的解<span class="math inline">\(\pmb{w}^*\)</span>和<span class="math inline">\(b^*\)</span>。<br>
设<span class="math inline">\(\pmb{\alpha}^* = (\alpha_1^*,\alpha_2^*,\cdots,\alpha_N^*)^T\)</span>是对偶最优化问题的解，则存在下标<span class="math inline">\(j\)</span>，使得<span class="math inline">\(\alpha_j^* \geq 0\)</span>，并可按下式求得原始最优化问题的解<span class="math inline">\(\pmb{w}^*,b^*\)</span>： <span class="math display">\[\begin{align}
    \pmb{w}^* &amp;= \sum_{i=1}^N \alpha_i^* y_i \pmb{x}_i \nonumber \\
    b^* &amp;= y_j - \sum_{i=1}^N \alpha_i^* y_i (\pmb{x}_i · \pmb{x}_j) \nonumber
\end{align}\]</span> <strong>证明</strong>：根据KKT条件成立，即得 <span class="math display">\[\begin{align}
    &amp; \nabla_{\pmb{w}} L(\pmb{w}^*,b^*,\pmb{\alpha}^*) = \pmb{w}^* - \sum_{i=1}^N \alpha_i^* y_i x_i = 0 \nonumber \\
    &amp; \nabla_{b} L(\pmb{w}^*,b^*,\pmb{\alpha}^*) = - \sum_{i=1}^N \alpha_i^* y_i = 0 \nonumber \\
    &amp; \alpha_i^*(y_i(\pmb{w}^*·\pmb{x}_i+b^*)-1) = 0, \quad i=1,2,\cdots,N \nonumber \\
    &amp; y_i (\pmb{w}^*·\pmb{x}_i + b^*) - 1 \geq 0, \quad i=1,2,\cdots,N \nonumber \\
    &amp; \alpha_i^* \geq 0, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 由此得 <span class="math display">\[\pmb{w}^* = \sum_{i=1}^N \alpha_i^* y_i \pmb{x}_i\]</span> 其中至少有一个<span class="math inline">\(\alpha_j^* &gt; 0\)</span>（反证法，假设<span class="math inline">\(\pmb{\alpha}^* = 0\)</span>，由对<span class="math inline">\(\pmb{w}\)</span>求偏导可知<span class="math inline">\(\pmb{w}^* = 0\)</span>，而<span class="math inline">\(\pmb{w}^* = 0\)</span>不是原始最优化问题的解，产生矛盾），对此<span class="math inline">\(j\)</span>有 <span class="math display">\[y_j (\pmb{w}^* · \pmb{x}_j + b^*) - 1 = 0\]</span> 又因为有<span class="math inline">\(y_j^2 = 1\)</span>，即得 <span class="math display">\[ b^* = y_j - \sum_{i=1}^N \alpha_i^* y_i (\pmb{x}_i · \pmb{x}_j)\]</span> 有上述最优解可以得到分离超平面可写成： <span class="math display">\[\sum_{i=1}^N \alpha_i^* y_i (\pmb{x} · \pmb{x}_i) + b^* = 0\]</span> 分类决策函数可写成： <span class="math display">\[f(\pmb{x}) = sign(\sum_{i=1}^N \alpha_i^* y_i (\pmb{x} · \pmb{x}_i) + b^*)\]</span> 从上式可以看出，分类决策函数只依赖于输入<span class="math inline">\(\pmb{x}\)</span>和训练样本输入的内积，上式称为线性可分支持向量机的对偶形式。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-26%20下午10.26.36.png">

</div>
<p>在线性可分支持向量机中，<span class="math inline">\(\pmb{w}^*\)</span>和<span class="math inline">\(b^*\)</span>只依赖于训练数据中对应于<span class="math inline">\(\alpha_i^* &gt; 0\)</span>的样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>，而其他样本点对<span class="math inline">\(\pmb{w}^*\)</span>和<span class="math inline">\(b^*\)</span>没有影响。将<strong>训练数据中对应于<span class="math inline">\(\alpha_i^*&gt;0\)</span>的实例点<span class="math inline">\(\pmb{x}_i \in \mathbf{R}^n\)</span>称为支持向量。</strong></p>
<h2 id="线性支持向量机与软间隔最大化">2. 线性支持向量机与软间隔最大化</h2>
<h3 id="线性支持向量机">2.1. 线性支持向量机</h3>
<p>线性可分问题的支持向量机学习方法不适用于线性不可分的训练数据，因为此时上述方法的不等式约束并不能都成立。这样就需要将硬间隔最大化修改为软间隔最大化。<br>
假设给定一个特征空间上的训练数据集 <span class="math display">\[T = \{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots, (\pmb{x}_N,y_N)\}\]</span> 其中，<span class="math inline">\(\pmb{x}_i \in \chi = \mathbf{R}^n\)</span>，<span class="math inline">\(y_i \in \{+1,-1\}, \quad i=1,2,\cdots,N\)</span>，<span class="math inline">\(\pmb{x}_i\)</span>为第<span class="math inline">\(i\)</span>个特征向量，也成为实例，<span class="math inline">\(y_i\)</span>为<span class="math inline">\(\pmb{x}_i\)</span>的类标记，并假设训练数据集不是线性可分的。通常情况是，训练数据中有一些特异点（outlier），将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。<br>
线性不可分意味着某些样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>不能满足函数间隔大于等于1的约束条件，为了解决此问题，可以对每个样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>引进一个松弛变量<span class="math inline">\(\xi_i \geq 0\)</span>，是函数间隔加上松弛变量大于等于1。此时，约束条件变为 <span class="math display">\[y_i(\pmb{w}·\pmb{x}_i + b) \geq 1 - \xi_i\]</span> 同样，对每个松弛变量<span class="math inline">\(\xi_i\)</span>，需要增加一个代价<span class="math inline">\(\xi_i\)</span>，目标函数变为 <span class="math display">\[\frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^N \xi_i\]</span> 这里，<span class="math inline">\(C&gt;0\)</span>为惩罚参数，一般由应用问题决定。<span class="math inline">\(C\)</span>值大时对误分类的惩罚增大，<span class="math inline">\(C\)</span>值小时对误分类的惩罚较小。最小化目标函数包含两层含义：使<span class="math inline">\(\frac{1}{2} ||\pmb{w}||^2\)</span>尽量小即间隔尽量大，同时使误分类点的个数尽量小，<span class="math inline">\(C\)</span>是调和二者的系数。<br>
线性不可分的线性支持向量机的学习问题变为如下凸二次规划问题（原始问题）： <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{w},b,\pmb{\xi}} \quad \frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^N \xi_i \nonumber \\
    &amp; s.t. \quad y_i(\pmb{w}·\pmb{x}_i + b) \geq 1 - \xi_i, \quad i=1,2,\cdots,N \nonumber \\
    &amp; \qquad \quad \xi_i \geq 0, \quad i=1,2,\cdots,N \nonumber 
\end{align}\]</span> 原始问题是一个凸二次规划问题，因而关于<span class="math inline">\((\pmb{w},b,\pmb{\xi})\)</span>的解是存在的。可以证明<span class="math inline">\(\pmb{w}\)</span>的解是唯一的，但<span class="math inline">\(b\)</span>的解可能不唯一，而是存在与一个区间。<br>
线性支持向量机包含线性可分支持向量机，由于现实中训练数据集往往是线性不可分的，线性支持向量机基友更广的适用性。<br>
<strong>线性支持向量机</strong>：对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题，得到的分离超平面为 <span class="math display">\[\pmb{w}^* · \pmb{x} + b^* = 0\]</span> 以及相应的分类决策函数 <span class="math display">\[f(x) = sign(\pmb{w}^* · \pmb{x} + b^*)\]</span> 称为支持向量机。</p>
<h3 id="学习的对偶算法-1">2.2. 学习的对偶算法</h3>
<p>原始问题的对偶问题： <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{\alpha}} \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) - \sum_{i=1}^N \alpha_i \nonumber \\
    &amp; s.t. \quad \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    &amp; \qquad \quad 0 \leq \alpha_i \leq C,quad i=1,2,\cdots,N \nonumber
\end{align}\]</span></p>
<p><strong>推导</strong>：<br>
原始最优化问题的拉格朗日函数为 <span class="math display">\[L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) = \frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i (y_i (\pmb{w}·\pmb{x}_i + b)-1 + \xi_i) - \sum_{i=1}^N \mu_i \xi_i\]</span> 对偶问题是拉格朗日函数的极大极小问题，首先求<span class="math inline">\(L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu})\)</span>对<span class="math inline">\(\pmb{w}, b, \pmb{\xi}\)</span>的极小： <span class="math display">\[\begin{align}
    \nabla_{\pmb{w}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) &amp;= \pmb{w} - \sum_{i=1}^N \alpha_i y_i \pmb{x}_i = 0 \nonumber \\
    \nabla_b L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) &amp;= - \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    \nabla_{\xi_i} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) &amp;= C - \alpha_i - \mu_i = 0 \nonumber
\end{align}\]</span> 求得 <span class="math display">\[\begin{equation}
    \pmb{w} = \sum_{i=1}^N \alpha_i y_i \pmb{x}_i \nonumber \\
    \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    C - \alpha_i - \mu_i = 0 \nonumber
\end{equation}\]</span></p>
<p>将所求等式带入拉格朗日函数得到 <span class="math display">\[\min_{\pmb{w},b,\pmb{\xi}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) = - \frac{1}{2} \sum_{i=1}^N \sum_{i=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) + \sum_{i=1}^N \alpha_i\]</span> 再对<span class="math inline">\(\min_{\pmb{w},b,\pmb{\xi}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu})\)</span>求<span class="math inline">\(\pmb{\alpha}\)</span>的极大，即得对偶问题： <span class="math display">\[\begin{align}
    &amp; \max_{\pmb{\alpha}} \quad - \frac{1}{2} \sum_{i=1}^N \sum_{i=1}^N \alpha_i \alpha_j y_i y_j (\pmb{x}_i · \pmb{x}_j) + \sum_{i=1}^N \alpha_i \nonumber \\
    &amp; s.t. \quad \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    &amp; \qquad \quad C - \alpha_i - \mu_i = 0 \nonumber \\
    &amp; \qquad \quad \alpha_i \geq 0 \nonumber \\
    &amp; \qquad \quad \mu_i \geq 0, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span> 将对偶最优化问题进行变换：利用等式约束<span class="math inline">\(C - \alpha_i - \mu_i = 0\)</span>消去<span class="math inline">\(\mu_i\)</span>，从而只留下变量<span class="math inline">\(\alpha_i\)</span>，并将后三个约束写成 <span class="math display">\[0 \leq \alpha_i \leq C\]</span> 再将对目标函数求极大转换为求极小，就得到了对偶问题。</p>
<p>设<span class="math inline">\(\pmb{\alpha}^* = (\alpha_1^*,\alpha_2^*,\cdots,\alpha_N^*)^T\)</span>是上述对偶问题的一个解，若存在<span class="math inline">\(\pmb{\alpha}^*\)</span>的一个分量<span class="math inline">\(\alpha_j^*\)</span>，<span class="math inline">\(0 &lt; \alpha_j^* &lt; C\)</span>，则原始问题的解<span class="math inline">\(\pmb{w}^*,b^*\)</span>可按下式求得： <span class="math display">\[\begin{align}
    \pmb{w}^* &amp;= \sum_{i=1}^N \alpha^* y_i \pmb{x}_i \nonumber \\
    b^* &amp;= y_j - \sum_{i=1}^N y_i \alpha_i^* (\pmb{x}_i · \pmb{x}_j) \nonumber
\end{align}\]</span></p>
<p><strong>证明</strong>：原始问题是凸二次规划问题，解满足KKT条件，即得 <span class="math display">\[\begin{equation}
    \nabla_{\pmb{w}} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) = \pmb{w} - \sum_{i=1}^N \alpha_i y_i \pmb{x}_i = 0 \nonumber \\
    \nabla_b L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) = - \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\
    \nabla_{\xi_i} L(\pmb{w},b,\pmb{\xi},\pmb{\alpha},\pmb{\mu}) = C - \alpha_i - \mu_i = 0 \nonumber \\
    \alpha^*(y_i(\pmb{w}^*·\pmb{x}_i + b^*)-1+\xi_i^*) = 0 \nonumber \\
    \mu_i^* \xi_i^* = 0 \nonumber \\
    y_i(\pmb{w}^* · \pmb{x}_i + b^*) - 1 + \xi_I^* \geq 0 \nonumber \\
    \xi_i^* \geq 0 \nonumber  \\
    \alpha_i^* \geq 0 \nonumber \\
    \mu_i^* \geq 0, \quad i=1,2,\cdots,N \nonumber
\end{equation}\]</span> 由上述等式可推得<span class="math inline">\(\pmb{w}^*, b^*\)</span>。<br>
分离超平面可写成 <span class="math display">\[\sum_{i=1}^N \alpha_i^* y_i (\pmb{x} · \pmb{x}_i) + b^* = 0\]</span> 分类决策面可写成 <span class="math display">\[f(x) = sign(\sum_{i=1}^N \alpha_i^* y_i (\pmb{x} · \pmb{x}_i) + b^*)\]</span></p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20上午10.32.24.png">

</div>
<h3 id="支持向量">2.3. 支持向量</h3>
<p>在线性不可分的情况下，将对偶问题的解<span class="math inline">\(\pmb{\alpha}^* = \{\alpha_1^*, \alpha_2^*, \cdots, \alpha_N^*\}^T\)</span>中对应于<span class="math inline">\(\alpha_i^* &gt; 0\)</span>的样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>的实例<span class="math inline">\(\pmb{x}_i\)</span>称为支持向量（软间隔的支持向量）。如下图，分离超平面由实线表示，间隔边界由虚线表示，正例点由<span class="math inline">\(\circ\)</span>表示，负例点由<span class="math inline">\(\times\)</span>表示。图中还标出了实例<span class="math inline">\(\pmb{x}_i\)</span>到间隔边界的距离<span class="math inline">\(\frac{\xi_i}{||\pmb{w}||}\)</span>。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20上午10.57.55.png">

</div>
<p>软间隔的支持向量<span class="math inline">\(\pmb{x}_i\)</span>或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧：</p>
<ul>
<li>若<span class="math inline">\(\alpha_i^* &lt; C\)</span>，则<span class="math inline">\(\xi_i = 0\)</span>，支持向量恰好落在间隔边界上；</li>
<li>若<span class="math inline">\(\alpha_i^* = C\)</span>，<span class="math inline">\(0 &lt; \xi_i &lt; 1\)</span>，则分类正确，<span class="math inline">\(\pmb{x}_i\)</span>在间隔边界与分离超平面之间；</li>
<li>若<span class="math inline">\(\alpha_i^* = C\)</span>，<span class="math inline">\(\xi_i = 1\)</span>，则<span class="math inline">\(\pmb{x}_i\)</span>在分离超平面上；</li>
<li>若<span class="math inline">\(\alpha_i^* = C\)</span>，<span class="math inline">\(\xi_i &gt; 1\)</span>，则<span class="math inline">\(\pmb{x}_i\)</span>位于分离超平面误分一侧。</li>
</ul>
<h3 id="合页损失函数">2.4. 合页损失函数</h3>
<p>线性支持向量机还有另外一种解释，就是最小化以下目标函数： <span class="math display">\[\sum_{i=1}^N [1 - y_i(\pmb{w}·\pmb{x}_i + b)]_+ \lambda ||\pmb{w}||^2\]</span> 目标函数的第一项是经验损失或经验风险，函数 <span class="math display">\[L(y(\pmb{w}·\pmb{x}+b)) = [1 - y(\pmb{w} ·\pmb{x}+b)]_+\]</span> 称为合页损失函数（hinge loss function）。下标&quot;<span class="math inline">\(+\)</span>&quot;表示以下取正值的函数： <span class="math display">\[[z]_+ = \begin{cases}
    z, \quad z&gt;0 \\
    0, \quad z \leq 0
\end{cases}\]</span> 由上式可以看出，当样本点<span class="math inline">\((\pmb{x}_i,y_i)\)</span>被正确分类且函数间隔（确信度）<span class="math inline">\(y_i(\pmb{w}·\pmb{x}_i+b)\)</span>大于1时，损失是0，否则损失是<span class="math inline">\(1-y_i(\pmb{w}·\pmb{x}_i+b)\)</span>。目标函数第二项是系数为<span class="math inline">\(\lambda\)</span>的<span class="math inline">\(\pmb{w}\)</span>的<span class="math inline">\(L_2\)</span>范数，是正则化项。</p>
<p>线性支持向量机原始最优化问题： <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{w},b,\pmb{\xi}} \quad \frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^N \xi_i \nonumber \\
    &amp; s.t. \quad y_i(\pmb{w}·\pmb{x}_i + b) \geq 1 - \xi_i, \quad i=1,2,\cdots,N \nonumber \\
    &amp; \qquad \quad \xi_i \geq 0, \quad i=1,2,\cdots,N \nonumber
\end{align}\]</span></p>
<p>等价于最优化问题 <span class="math display">\[\min_{\pmb{w},b} \quad \sum_{i=1}^N [1 - y_i(\pmb{w}·\pmb{x}_i + b)]_+ \lambda ||\pmb{w}||^2\]</span></p>
<p><strong>证明</strong>：令 <span class="math display">\[[1 - y_i(\pmb{w}·\pmb{x}_i+b)]_+ = \xi_i\]</span></p>
<p>当<span class="math inline">\(1-y_i(\pmb{w}·\pmb{x}_i+b)&gt;0\)</span>时，有<span class="math inline">\(y_i(\pmb{w}·\pmb{x}_i+b) = 1-\xi_i\)</span>；当<span class="math inline">\(1-y_i(\pmb{w}·\pmb{x}_i+b) \leq 0\)</span>时，<span class="math inline">\(\xi_i=0\)</span>，有<span class="math inline">\(y_i(\pmb{w}·\pmb{x}_i+b) \geq 1-\xi_i\)</span>。所以有<span class="math inline">\(y_i(\pmb{w}·\pmb{x}_i + b) \geq 1 - \xi_i, \quad i=1,2,\cdots,N\)</span>成立。所以有 <span class="math inline">\(\pmb{w},b,\xi_i\)</span>满足约束条件，所以最优问题可写成 <span class="math display">\[\min_{\pmb{w},b} \quad \sum_{i=1}^N \xi_i + \lambda ||\pmb{w}||^2\]</span> 若取<span class="math inline">\(\lambda = \frac{1}{2C}\)</span>，则 <span class="math display">\[\min_{\pmb{w},b} \quad \frac{1}{C} (\frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^N \xi_i)\]</span> 与原始最优化问题等价。<br>
合页损失函数图形如下图所示，横轴是函数间隔<span class="math inline">\(y(\pmb{w}·\pmb{x}+b)\)</span>，纵轴是损失。由于函数形状是一个合页，所以称为合页损失函数。合页损失函数不仅要分类正确，而且确信度足够高时损失才是0。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20下午10.05.11.png">

</div>
<p>图中还画出0-1损失函数，可以认为它是二分类问题真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化尤其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。此时的上界损失函数又称为代理损失函数（surrogate loss function）。<br>
图中虚线是感知机的损失函数<span class="math inline">\([-y_i(\pmb{w}·\pmb{x}_i+b)]_+\)</span>。</p>
<h2 id="非线性支持向量机与核函数">3. 非线性支持向量机与核函数</h2>
<h3 id="核技巧">3.1. 核技巧</h3>
<h4 id="非线性问题">3.1.1. 非线性问题</h4>
<p>对于给定的一个训练数据集<span class="math inline">\(T=\{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots,(\pmb{x}_N,y_N)\}\)</span>，其中，实例<span class="math inline">\(\pmb{x}_i\)</span>属于输入空间，<span class="math inline">\(\pmb{x}_i \in \chi = \mathbf{R}^n\)</span>，对应标记有两类<span class="math inline">\(y_i \in \{-1,+1\}, \quad i=1,2,\cdots,N\)</span>。如果能用<span class="math inline">\(\mathbf{R}^n\)</span>中的一个超平面将正负例正确分开，则称这个问题为非线性可分问题。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-02-28%20下午10.57.26.png">

</div>
<p>非线性问题往往不好求解，所以希望通过解线性分类问题的方法解决此问题。所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。<br>
用线性分类方法求解非线性分类问题分为两步：</p>
<ul>
<li>首先使用一个变换将原空间的数据映射到新空间；</li>
<li>在新空间里用线性分类学习方法从训练数据中学习分类模型。</li>
</ul>
<p>核技巧应用到支持向量机的基本想法：通过一个非线性变换将输入空间（欧式空间<span class="math inline">\(\mathbf{R}^n\)</span>或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间的超曲面模型对应于特征空间中的超平面模型。</p>
<h4 id="核函数">3.1.2. 核函数</h4>
<p><strong>核函数</strong>：设<span class="math inline">\(\chi\)</span>是输入空间（欧式空间<span class="math inline">\(\mathbf{R}^n\)</span>的子集或离散集合），又设<span class="math inline">\(\mathscr{H}\)</span>为特征空间（希尔伯特空间），如果存在一个从<span class="math inline">\(\chi\)</span>到<span class="math inline">\(\mathscr{H}\)</span>的映射： <span class="math display">\[\phi(\pmb{x}):\chi \to \mathscr{H}\]</span> 使得所有<span class="math inline">\(\pmb{x},\pmb{z} \in \chi\)</span>，函数<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>满足条件 <span class="math display">\[K(\pmb{x},\pmb{z}) = \phi(\pmb{x}) · \phi(\pmb{z})\]</span> 则称<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>为核函数，<span class="math inline">\(\phi(\pmb{x})\)</span>为映射函数，式中<span class="math inline">\(\phi(\pmb{x}) · \phi(\pmb{z})\)</span>为内积。<br>
核技巧的想法：在学习与预测中只定义核函数<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>，而不显式地定义映射函数<span class="math inline">\(\phi\)</span>。通常，直接计算<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>比较容易，而通过<span class="math inline">\(\phi(\pmb{x})\)</span>和<span class="math inline">\(\phi(\pmb{x},\pmb{z})\)</span>并不容易。<br>
<strong>注意</strong>：<span class="math inline">\(\phi\)</span>是输入空间<span class="math inline">\(\mathbf{R}^n\)</span>到特征空间<span class="math inline">\(\mathscr{H}\)</span>的映射，特征空间<span class="math inline">\(\mathscr{H}\)</span>一般是高维的，甚至是无穷维的。对于给定的<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>，特征空间<span class="math inline">\(\mathscr{H}\)</span>和映射函数<span class="math inline">\(\phi\)</span>的取法并不唯一，可以取不同的特征空间，即便是在统一特征空间里也可以取不同的映射。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-01%20上午10.36.25.png">

</div>
<h4 id="核技巧在支持向量机中的应用">3.1.3. 核技巧在支持向量机中的应用</h4>
<p>在线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积。在对偶问题的目标函数中的内积<span class="math inline">\(\pmb{x}_i·\pmb{x}_j\)</span>可以用核函数<span class="math inline">\(K(\pmb{x}_i,\pmb{x}_j) = \phi(\pmb{x}_i)·\phi(\pmb{x}_j)\)</span>来代替，此时对偶问题的目标函数为 <span class="math display">\[W(\pmb{\alpha}) = \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(\pmb{x}_i, \pmb{x}_j) - \sum_{i=1}^N \alpha_i\]</span> 同样，分类决策函数中的内积也可以用核函数代替： <span class="math display">\[f(x) = sign(\sum_{i=1}^{N_s} \alpha_i^* y_i \phi(\pmb{x}_i)·\phi(\pmb{x})+b^*) = sign(\sum_{i=1}^{N_s} \alpha_i^* y_i K(\pmb{x}_i, \pmb{x}_j)+b^*)\]</span> 上式等价于经过映射函数<span class="math inline">\(\phi\)</span>将原来的输入空间变换到一个新的特征空间，将输入空间的内积<span class="math inline">\(\pmb{x}_i·\pmb{x}_j\)</span>变换为特征空间中的内积<span class="math inline">\(\phi(\pmb{x}_i)·\phi(\pmb{x})\)</span>。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性模型。<br>
在核函数<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。</p>
<h3 id="正定核">3.2. 正定核</h3>
<p>通常所说的核函数就是正定核函数（positive definite kernel function）。假设<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>是定义在<span class="math inline">\(\chi \times \chi\)</span>上的对称函数，并且对任意的<span class="math inline">\(\pmb{x}_1,\pmb{x}_2,\cdots,\pmb{x}_m \in \chi\)</span>，<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>关于<span class="math inline">\(\pmb{x}_1,\pmb{x}_2,\cdots,\pmb{x}_m\)</span>的Gram矩阵式半正定的，可以依据函数<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>，构成一个希尔伯特空间。<br>
步骤：</p>
<ol style="list-style-type: decimal">
<li>首先定义映射<span class="math inline">\(\phi\)</span>并构成向量空间<span class="math inline">\(\mathscr{S}\)</span>；</li>
<li>在<span class="math inline">\(\mathscr{S}\)</span>上定义内积构成内积空间；</li>
<li>将<span class="math inline">\(\mathscr{S}\)</span>完备化构成希尔伯特空间。</li>
</ol>
<p><strong>正定核的充要条件</strong>：设<span class="math inline">\(K:\chi \times \chi \to \mathbf{R}\)</span>是对称函数，则<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>为正定核函数的充要条件：对任意<span class="math inline">\(\pmb{x}_i \in \chi, \quad i=1,2,\cdots,m\)</span>，<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>对应的Gram矩阵<span class="math inline">\(K = [K(\pmb{x}_i,\pmb{x}_j)]_{m \times m}\)</span>是半正定矩阵。</p>
<h3 id="常用核函数">3.3. 常用核函数</h3>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-01%20下午4.25.04.png">

</div>
<h3 id="非线性支持向量机">3.4. 非线性支持向量机</h3>
<p><strong>非线性支持向量机</strong>：从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划，学习得到的分类决策函数 <span class="math display">\[f(x) = sign(\sum_{i=1}^N \alpha_i^* y_i K(\pmb{x}·\pmb{x}_i)+b^*)\]</span> 称为非线性支持向量机，<span class="math inline">\(K(\pmb{x},\pmb{z})\)</span>是正定核函数。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-01%20下午4.29.22.png">

</div>
<h2 id="支持向量回归">4. 支持向量回归</h2>
<p>支持向量回归（support vector regression，SVR）就是找到一个回归面，让一个集合的所有数据到该平面的距离最近。<br>
对于样本<span class="math inline">\((\pmb{x},y)\)</span>，传统回归模型通常直接基于模型输出<span class="math inline">\(f(\pmb{x})\)</span>与真实标记<span class="math inline">\(y\)</span>之间的差别来计算损失，当且仅当<span class="math inline">\(f(\pmb{x})\)</span>与<span class="math inline">\(y\)</span>完全相同时，损失才为0.而SVR可以容忍<span class="math inline">\(f(\pmb{x})\)</span>与<span class="math inline">\(y\)</span>之间最多有<span class="math inline">\(\epsilon\)</span>的偏差，即仅当<span class="math inline">\(f(\pmb{x})\)</span>与<span class="math inline">\(y\)</span>之间的差别绝对值大于<span class="math inline">\(\epsilon\)</span>时才计算损失。如下图所示，相当于以<span class="math inline">\(f(\pmb{x})\)</span>为中心，构建了一个宽度为<span class="math inline">\(2 \epsilon\)</span>的间隔带，若样本落入此间隔带内就认为是预测正确的。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-03%20下午10.21.05.png">

</div>
<p>SVR问题可形式化为 <span class="math display">\[\min_{\pmb{w},b} \frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^m \mathscr{l}_{\epsilon} (f(\pmb{x}_i) - y_i)\]</span> 其中<span class="math inline">\(C\)</span>为正则化常数，<span class="math inline">\(\mathscr{l}_{\epsilon}\)</span>是下图所示的<span class="math inline">\(\epsilon\)</span>-不敏感损失(<span class="math inline">\(\epsilon\)</span>-insensitive loss)函数 <span class="math display">\[\mathscr{l}_{\epsilon} = 
\begin{cases}
    0, \quad if |z| \leq \epsilon \\
    |z| - \epsilon, \quad otherwise
\end{cases}\]</span></p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-03%20下午10.38.51.png">

</div>
<p>引入松弛变量<span class="math inline">\(\xi_i\)</span>和<span class="math inline">\(\hat{\xi}_i\)</span>，将上式重写为 <span class="math display">\[\begin{align}
    &amp; \min_{\pmb{w},b,\xi_i,\hat{\xi}_i} \quad \frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^m(\xi_i + \hat{\xi}_i) \nonumber \\
    &amp; \quad s.t. \quad f(\pmb{x}_i) - y_i \leq \epsilon + \xi_i, \nonumber \\
    &amp; \qquad \qquad y_i - f(\pmb{x}) \leq \epsilon + \hat{\xi}_i, \nonumber \\
    &amp; \qquad \qquad \xi_i \geq 0, \quad \hat{\xi}_i \geq 0, \quad i=1,2,\cdots,m \nonumber
\end{align}\]</span> 通过引入拉格朗日乘子<span class="math inline">\(\mu_i \geq 0, \quad \hat{\mu}_i \geq 0, \quad \alpha_i \geq 0, \quad \hat{\alpha}_i \geq 0\)</span>，得到拉格朗日函数 <span class="math display">\[L(\pmb{w},b,\pmb{\alpha},\hat{\pmb{\alpha}},\pmb{\xi},\hat{\pmb{\xi}},\pmb{\mu},\hat{\pmb{\mu}}) = \frac{1}{2} ||\pmb{w}||^2 + C \sum_{i=1}^m (\xi_i + \hat{\xi}_i) - \sum_{i=1}^m \mu_i \xi_i - \sum_{i=1}^m \hat{\mu}_i \hat{\xi}_i + \sum_{i=1}^m \alpha_i(f(\pmb{x}_i)-y_i-\epsilon-\xi_i) - \sum_{i=1}^m \hat{\xi}_i (y_i-f(\pmb{x}_i)-\epsilon-\hat{\xi}_i)\]</span> 令<span class="math inline">\(L(\pmb{w},b,\pmb{\alpha},\hat{\pmb{\alpha}},\pmb{\xi},\hat{\pmb{\xi}},\pmb{\mu},\hat{\pmb{\mu}})\)</span>对<span class="math inline">\(\pmb{w},b,\xi_i,\hat{\xi}_i\)</span>的偏导为0可得 <span class="math display">\[\begin{align}
    \pmb{w} &amp;= \sum_{i=1}^m (\hat{\alpha}_i - \alpha_i) \pmb{x}_i \nonumber \\
    0 &amp;= \sum_{i=1}^m (\hat{\alpha}_i - \alpha_i) \nonumber \\
    C &amp;= \alpha_i + \mu_i \nonumber \\
    C &amp;= \hat{\alpha}_i + \hat{\mu}_i \nonumber
\end{align}\]</span></p>
<p>带入拉格朗日函数中得到 <span class="math display">\[\begin{align}
    &amp; \max_{\pmb{\alpha},\hat{\pmb{\alpha}}} \quad \sum_{i=1}^m y_i(\hat{\alpha}_i - \alpha_i) - \epsilon (\hat{\alpha}_i + \alpha_i) - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m(\hat{\alpha}_i - \alpha_i)(\hat{\alpha}_j - \alpha_j) \pmb{x}_i^T \pmb{x}_j \nonumber \\
    &amp; s.t. \quad \sum_{i=1}^m (\hat{\alpha}_i - \alpha_i) = 0, \nonumber \\
    &amp; \qquad \quad 0 \leq \alpha_i, \hat{\alpha}_i \leq C \nonumber
\end{align}\]</span> 上述过程需要满足KKT条件，即 <span class="math display">\[\begin{cases}
    \alpha_i(f(\pmb{x}_i) - y_i - \epsilon - \xi_i) = 0, \nonumber \\
    \hat{\alpha}_i(y_i - f(\pmb{x}_i) - \epsilon - \hat{\xi}_i) = 0, \nonumber \\
    \alpha_i \hat{\alpha}_i = 0, \quad \xi_i \hat{\xi}_i = 0, \nonumber \\
    (C - \alpha_i) \xi_i = 0, \quad (C - \hat{\alpha}_i) \hat{\xi}_i = 0. \nonumber
\end{cases}\]</span> 可以看出，仅当样本<span class="math inline">\((\pmb{x}_i,y_i)\)</span>不落入<span class="math inline">\(\epsilon\)</span>-间隔带中，相应的<span class="math inline">\(\alpha_i\)</span>和<span class="math inline">\(\hat{\alpha}_i\)</span>才能取得非零值。<br>
将<span class="math inline">\(\pmb{w} = \sum_{i=1}^m (\hat{\alpha}_i - \alpha_i) \pmb{x}_i\)</span>带入，则SVR的解形如 <span class="math display">\[f(\pmb{x}) = \sum_{i=1}^m (\hat{\alpha} - \alpha_i) \pmb{x}_i^T \pmb{x} + b\]</span> 能使上式中<span class="math inline">\((\hat{\alpha}_i - \alpha_i) \neq 0\)</span>的样本即为SVR的支持向量，它们必然落在<span class="math inline">\(\epsilon\)</span>-间隔带外。显然，SVR的支持向量仅是训练样本的一部分，其解仍具有洗属性。<br>
有KKT条件可以看出，在得到<span class="math inline">\(\alpha_i\)</span>之后，若<span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>，则必有<span class="math inline">\(\xi_i=0\)</span>，进而有 <span class="math display">\[b = y_i + \epsilon - \sum_{j=1}^m (\hat{\alpha}_j - \alpha_j) \pmb{x}_j^T \pmb{x}_i\]</span> 理论上来说，可任取满足<span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>的样本通过上式求解<span class="math inline">\(b\)</span>，但在实践中采取一种更鲁棒的方法：选取多个（或所有）满足条件<span class="math inline">\(0 &lt; \alpha_i &lt; C\)</span>的样本求解<span class="math inline">\(b\)</span>后取平均值。<br>
如果考虑特征映射形式，则 <span class="math display">\[\pmb{w} = \sum_{i=1}^m (\hat{\alpha}_i - \alpha_i) \phi(\pmb{x}_i)\]</span> 则SVR可表示为 <span class="math display">\[f(\pmb{x}) = \sum_{i=1}^m (\hat{\alpha}_i - \alpha_i) K(\pmb{x},\pmb{x}_i) + b\]</span> 其中K(·)为核函数。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/25/机器学习/决策树/" rel="next" title="决策树">
                <i class="fa fa-chevron-left"></i> 决策树
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/05/机器学习/SMO-序列最小最优化算法/" rel="prev" title="SMO-序列最小最优化算法">
                SMO-序列最小最优化算法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">silen Zhou</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">38</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#支持向量机"><span class="nav-number">1.</span> <span class="nav-text">支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性可分支持向量机与硬间隔最大化"><span class="nav-number">1.1.</span> <span class="nav-text">1. 线性可分支持向量机与硬间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性可分支持向量机"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1. 线性可分支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#函数间隔和几何间隔"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2. 函数间隔和几何间隔</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#间隔最大化"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3. 间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#最大间隔分离超平面"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">1.3.1. 最大间隔分离超平面</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#最大间隔分离超平面的存在唯一性"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">1.3.2. 最大间隔分离超平面的存在唯一性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#支持向量和间隔边界"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">1.3.3. 支持向量和间隔边界</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习的对偶算法"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4. 学习的对偶算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性支持向量机与软间隔最大化"><span class="nav-number">1.2.</span> <span class="nav-text">2. 线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性支持向量机"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1. 线性支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#学习的对偶算法-1"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2. 学习的对偶算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3. 支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#合页损失函数"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4. 合页损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#非线性支持向量机与核函数"><span class="nav-number">1.3.</span> <span class="nav-text">3. 非线性支持向量机与核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#核技巧"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1. 核技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#非线性问题"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">3.1.1. 非线性问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核函数"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">3.1.2. 核函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核技巧在支持向量机中的应用"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">3.1.3. 核技巧在支持向量机中的应用</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正定核"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2. 正定核</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常用核函数"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3. 常用核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非线性支持向量机"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.4. 非线性支持向量机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量回归"><span class="nav-number">1.4.</span> <span class="nav-text">4. 支持向量回归</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">silen Zhou</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.2</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.2"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
