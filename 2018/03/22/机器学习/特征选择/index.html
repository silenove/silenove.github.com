<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-EN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.2" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.0.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  
  <meta name="keywords" content="机器学习,预处理," />


<meta name="description" content="特征选择 1. 子集搜索与评价 对于一个学习任务来说，给定属性集，其中有些属性可能很关键，而另一些属性则可能没什么用。这里将属性称为“特征”，对当前学习任务有用的属性称为“相关特征”（relevant feature），没什么用的属性称为“无关特征”（irrelevant feature），从给定的特征集合中选择出相关特征子集的过程，称为特征选择（feature selection）。 特征选择是">
<meta name="keywords" content="机器学习,预处理">
<meta property="og:type" content="article">
<meta property="og:title" content="特征选择">
<meta property="og:url" content="http://yoursite.com/2018/03/22/机器学习/特征选择/index.html">
<meta property="og:site_name" content="silenove blogs">
<meta property="og:description" content="特征选择 1. 子集搜索与评价 对于一个学习任务来说，给定属性集，其中有些属性可能很关键，而另一些属性则可能没什么用。这里将属性称为“特征”，对当前学习任务有用的属性称为“相关特征”（relevant feature），没什么用的属性称为“无关特征”（irrelevant feature），从给定的特征集合中选择出相关特征子集的过程，称为特征选择（feature selection）。 特征选择是">
<meta property="og:locale" content="zh-EN">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-22%20下午1.18.13.png">
<meta property="og:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-22%20下午1.49.53.png">
<meta property="og:updated_time" content="2018-03-22T10:12:07.003Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="特征选择">
<meta name="twitter:description" content="特征选择 1. 子集搜索与评价 对于一个学习任务来说，给定属性集，其中有些属性可能很关键，而另一些属性则可能没什么用。这里将属性称为“特征”，对当前学习任务有用的属性称为“相关特征”（relevant feature），没什么用的属性称为“无关特征”（irrelevant feature），从给定的特征集合中选择出相关特征子集的过程，称为特征选择（feature selection）。 特征选择是">
<meta name="twitter:image" content="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-22%20下午1.18.13.png">






  <link rel="canonical" href="http://yoursite.com/2018/03/22/机器学习/特征选择/"/>


  <title>特征选择 | silenove blogs</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-EN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">silenove blogs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">沿路旅程如歌褪变</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Beranda</a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Arsip</a>
        </li>
      

      
    </ul>
  

  
</nav>


  



 </div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/22/机器学习/特征选择/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="silen Zhou">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="silenove blogs">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">特征选择</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Diposting di</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-22T18:11:22+08:00">2018-03-22</time>
            

            
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="特征选择">特征选择</h1>
<h2 id="子集搜索与评价">1. 子集搜索与评价</h2>
<p>对于一个学习任务来说，给定属性集，其中有些属性可能很关键，而另一些属性则可能没什么用。这里将属性称为“特征”，对当前学习任务有用的属性称为“相关特征”（relevant feature），没什么用的属性称为“无关特征”（irrelevant feature），从给定的特征集合中选择出相关特征子集的过程，称为<strong>特征选择（feature selection）</strong>。</p>
<p>特征选择是一个重要的数据预处理过程，现实任务中，获得数据后通常先进行特征选择，此后再训练学习器。进行特征选择有两个重要原因：</p>
<ul>
<li>现实任务中经常遇到维数灾难的问题，若能从过多的特征中选择出重要特征，会使后续学习过程仅需要在一部分特征上构建模型，维数灾难问题会大为减轻，这与降维有相似的动机；</li>
<li>去除不相关特征往往会降低学习任务的难度，只留下关键特征，学习效果可能更好。</li>
</ul>
<p>特征选择必须保证不丢失重要特征，否则后续学习过程会因为重要信息丢失而无法获得好的性能。给定数据集，根据学习任务的不同，相关特征也很可能不同，特征选择中的“无关特征”是指与当前学习任务无关。有一类特征称为“冗余特征”（redundant feature），其所包含的信息可以从其他特征中推演过来，冗余特征在很多时候不起作用，去除它们会减轻学习过程的负担。例如，若已知“底面长”和“底面宽”，则“底面积”则是冗余特征。但是当学习任务是估算立方体体积，“底面积”这个冗余特征的存在将使体积的估算更为容易，即冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的。为简化说明，假定下述讨论不涉及冗余特征。</p>
<p>特征选择的两个关键环节：</p>
<ul>
<li>子集搜索</li>
<li>子集评价</li>
</ul>
<h3 id="子集搜索">1.1. 子集搜索</h3>
<p>搜索策略：</p>
<ol style="list-style-type: decimal">
<li><strong>前向搜索（forward）</strong>：给定特征集合<span class="math inline">\(\{a_1,a_2,\cdots,a_d\}\)</span>，可将每个特征看做一个候选子集，对这<span class="math inline">\(d\)</span>个候选单特征子集进行评价，假定<span class="math inline">\(\{a_2\}\)</span>最优，于是将<span class="math inline">\(\{a_2\}\)</span>作为第一轮的选定集；然后，在上一轮的选定集中加入一个特征，构成包含两个特征的候选子集，假定在这<span class="math inline">\(d-1\)</span>个候选两特征子集中<span class="math inline">\(\{a_2,a_4\}\)</span>最优，且优于<span class="math inline">\(\{a_2\}\)</span>，于是将<span class="math inline">\(\{a_2,a_4\}\)</span>作为本轮的选定集，<span class="math inline">\(\cdots \cdots\)</span>假定在第<span class="math inline">\(k+1\)</span>轮时，最优的候选<span class="math inline">\((k+1)\)</span>特征子集不如上一轮的选定集，则停止生成候选子集，并将上一轮选定的<span class="math inline">\(k\)</span>特征集合作为特征选择结果。这是一种逐渐增加相关特征的策略；</li>
<li><strong>后向搜索（backward）</strong>：与前向搜索类似，若从完整的特征集合开始，每次尝试去掉一个无关特征，这样逐渐减少特征的策略称为后向搜索；</li>
<li><strong>双向搜索（bidirectional）</strong>：将前向与后向搜索结合起来，每一轮逐渐增加选定相关特征（这些特征在后续轮中将确定不会被移除）、同时减少无关特征，这样的策略称为双向搜索。</li>
</ol>
<p>显然，上述策略都是贪心的，它们仅考虑了使本轮选定集最优的情况。例如，在第三轮中假定选择<span class="math inline">\(a_5\)</span>优于<span class="math inline">\(a_6\)</span>，于是选定集为<span class="math inline">\(\{a_2,a_4,a_5\}\)</span>，然而在第四轮却可能是<span class="math inline">\(\{a_2,a_4,a_6,a_8\}\)</span>比所有的<span class="math inline">\(\{a_2,a_4,a_5,a_i\}\)</span>都更优。遗憾的是，若不进行穷举搜索，这样的问题无法避免。</p>
<h3 id="子集评价">1.2. 子集评价</h3>
<p>给定数据集<span class="math inline">\(D\)</span>，假定<span class="math inline">\(D\)</span>中第<span class="math inline">\(i\)</span>类样本所占的比例为<span class="math inline">\(p_i(i=1,2,\cdots,|\mathscr{Y}|)\)</span>。为了便于讨论，假定样本属性均为离散型。对属性子集<span class="math inline">\(A\)</span>，假定根据其取值将<span class="math inline">\(D\)</span>分成了<span class="math inline">\(V\)</span>个子集<span class="math inline">\(\{D^1,D^2,\cdots,D^V\}\)</span>，每个子集中的样本在<span class="math inline">\(A\)</span>上取值相同，于是我们可计算属性子集<span class="math inline">\(A\)</span>的信息增益： <span class="math display">\[Gain(A) = Ent(D) - \sum_{v=1}^V \frac{|D^v|}{|D|} Ent(D^v)\]</span> 其中信息熵定义为 <span class="math display">\[Ent(D) = - \sum_{k=1}^{\mathscr{Y}} p_k \log_2 p_k\]</span> 信息增益<span class="math inline">\(Gain(A)\)</span>越大，意味着特征子集<span class="math inline">\(A\)</span>包含的有助于分类的信息越多。于是，对于每个候选特征子集，我们可基于训练数据集<span class="math inline">\(D\)</span>来计算其信息增益，以此作为评价准则。<br>
更一般的，特征子集<span class="math inline">\(A\)</span>实际上确定了对数据集<span class="math inline">\(D\)</span>的一个划分，每个划分区域对应着<span class="math inline">\(A\)</span>上的一个取值，而样本标记信息<span class="math inline">\(Y\)</span>对应着对<span class="math inline">\(D\)</span>的真实划分，通过估算这两个划分的差异，就能对<span class="math inline">\(A\)</span>进行评价。与<span class="math inline">\(Y\)</span>对应的划分的差异越小，则说明<span class="math inline">\(A\)</span>越好。信息熵仅是判断这个差异的一种途径，其他能判断两个划分差异的机制都能用于特征子集的评价。<br>
将特征子集搜索机制与子集评价机制相结合，即可得到特征选择方法。例如将前向搜索与信息熵相结合，这显然与决策树算法非常相似。事实上，决策树可用于特征选择，树节点的划分属性所组成的集合就是选择出的特征子集。其他的特征选择方法未必像决策树特征选择这么明显，但它们本质上都是显式或隐式地结合了某种（或多种）子集搜索机制和子集评价机制。</p>
<p>常见的特征选择方法大致分为三类：</p>
<ul>
<li>过滤式（filter）；</li>
<li>包裹式（wrapper）；</li>
<li>嵌入式（embedding）。</li>
</ul>
<h2 id="过滤式选择">2. 过滤式选择</h2>
<p>过滤式方法先对数据集进行特征选择，然后再训练学习器，特征选择过程与后续学习器无关。这相当于先用特征选择过程对初始特征进行“过滤”，再用过滤后的特征来训练模型。<br>
<strong>Relief</strong>是一种著名的过滤式特征选择方法，该方法设计了一个“相关统计量”来度量特征的重要性。该统计量是一个向量，其每个分量分别对应于一个初始特征，而特征子集的重要性则是由子集中每个特征所对应的相关统计量分量之和来决定。于是，最终只需指定一个阈值<span class="math inline">\(\tau\)</span>，然后选择比<span class="math inline">\(\tau\)</span>大的相关统计量分量对应的特征即可；也可指定欲选取的特征个数<span class="math inline">\(k\)</span>，然后选择相关统计量分量最大的<span class="math inline">\(k\)</span>个特征。<br>
显然，Relief算法的关键是如何确定相关统计量。给定训练集<span class="math inline">\(\{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2), \cdots,(\pmb{x}_m,y_m)\}\)</span>，对每个示例<span class="math inline">\(\pmb{x}_i\)</span>，Relief先在<span class="math inline">\(\pmb{x}_i\)</span>的同类样本中寻找其最近邻<span class="math inline">\(\pmb{x}_{i,nh}\)</span>，称为“猜中近邻”（near-hit），再从<span class="math inline">\(\pmb{x}_i\)</span>的异类样本中寻找其最近邻<span class="math inline">\(\pmb{x}_{i,nm}\)</span>，称为“猜错近邻”（near-miss），然后，相关统计量对应于属性<span class="math inline">\(j\)</span>的分量为： <span class="math display">\[\delta^j = \sum_i - diff(x_i^j,x_{i,nh}^j)^2 + diff(x_i^j, x_{i,nm}^j)^2\]</span> 其中<span class="math inline">\(x_i^j\)</span>表示样本<span class="math inline">\(\pmb{x}_a\)</span>在属性<span class="math inline">\(j\)</span>上的取值，<span class="math inline">\(diff(x_a^j,x_b^j)\)</span>取决于属性<span class="math inline">\(j\)</span>的类型；若属性<span class="math inline">\(j\)</span>为离散型，则<span class="math inline">\(x_a^j = x_b^j\)</span>时<span class="math inline">\(diff(x_a^j,x_b^j) = 0\)</span>，否则为<span class="math inline">\(1\)</span>；若属性<span class="math inline">\(j\)</span>为连续型，则<span class="math inline">\(diff(x_a^j,x_b^j) = |x_a^j - x_b^j|\)</span>，注意<span class="math inline">\(x_a^j,x_b^j\)</span>已经规范化到<span class="math inline">\([0,1]\)</span>区间。</p>
<p>从上式可看出，若<span class="math inline">\(\pmb{x}_i\)</span>与其猜中近邻<span class="math inline">\(\pmb{x}_{i,nh}\)</span>在属性<span class="math inline">\(j\)</span>上的距离小于<span class="math inline">\(\pmb{x}_i\)</span>与其猜错近邻<span class="math inline">\(\pmb{x}_{i,nm}\)</span>的距离，则说明属性<span class="math inline">\(j\)</span>对区分同类与异类样本是有益的，于是增大属性<span class="math inline">\(j\)</span>所对应的统计量分量；反之，若<span class="math inline">\(\pmb{x}_i\)</span>与其猜中近邻<span class="math inline">\(\pmb{x}_{i,nh}\)</span>在属性<span class="math inline">\(j\)</span>上的距离大于<span class="math inline">\(\pmb{x}_i\)</span>与其猜错近邻<span class="math inline">\(\pmb{x}_{i,nm}\)</span>的距离，则说明属性<span class="math inline">\(j\)</span>起负面作用，于是减小属性<span class="math inline">\(j\)</span>所对应的统计量分量。最后，对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量，分量值越大，则对应属性的分类能力就越强。<br>
公式中的<span class="math inline">\(i\)</span>指出了用于平均的样本下标，实际上Relief算法只需在数据集的采样上而不必在整个数据集上估计相关变量。显然，Relief的时间开销随采样次数以及原始特征数线性增长，因此是一个运行效率很高的过滤式特征选择算法。</p>
<p>Relief算法是为二分类问题设计的，其扩展变体Relief-F能处理多分类问题。假定数据集<span class="math inline">\(D\)</span>中的样本来自<span class="math inline">\(|\mathscr{Y}|\)</span>个类别。对示例<span class="math inline">\(\pmb{x}_i\)</span>，若它属于第<span class="math inline">\(k\)</span>类<span class="math inline">\((k \in \{1,2,\cdots,|\mathscr{Y}|\})\)</span>，则Relief-F算法先在第<span class="math inline">\(k\)</span>类的样本中寻找<span class="math inline">\(\pmb{x}_i\)</span>的最近邻示例<span class="math inline">\(\pmb{x}_{i,nh}\)</span>并将其作为猜中近邻，然后在第<span class="math inline">\(k\)</span>类之外的每个类中找到一个<span class="math inline">\(\pmb{x}_i\)</span>的最近邻示例作为猜错近邻，即为<span class="math inline">\(\pmb{x}_{i,l,nm} (l = 1,2,\cdots,|\mathscr{Y}|; l \neq k)\)</span>。于是，相关统计量对应于属性<span class="math inline">\(j\)</span>的分量为 <span class="math display">\[\delta^j = \sum_i - diff(x_i^j,x_{i,nh}^j)^2 + \sum_{l \neq k} (p_l \times diff(x_i^j, x_{i,l,nm}^j)^2)\]</span> 其中<span class="math inline">\(p_l\)</span>为第<span class="math inline">\(l\)</span>类样本在数据集<span class="math inline">\(D\)</span>中所占的比例。</p>
<h2 id="包裹式选择">3. 包裹式选择</h2>
<p>与过滤式特征选择不考虑后续学习器不同，包裹式特征选择直接把最终将要使用的学习器的性能作为特征子集的评价准则，包裹式特征选择的目的就是为给定学习器选择最有利于其性能、“量身定做”的特征子集。<br>
一般而言，由于包裹式特征选择方法直接针对给定学习器进行优化，因此从最终学习器性能来看，包裹式特征选择比过滤式特征选择更好。但另一方面，由于在特征选择过程中需多次训练学习器，因此包裹式特征选择的计算开销通常比过滤式特征选择大的多。<br>
<strong>LVW（Las Vegas Wrapper）</strong>是一个典型的包裹式特征选择方法。它在拉斯维加斯方法（Las Vegas method）框架下使用随机策略进行子集搜索，并以最终分类器的误差为特征子集评价准则，算法如下：</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-22%20下午1.18.13.png">

</div>
<p>需要注意的是，由于LVW算法中特征子集搜索采用了随机策略，而每次特征子集评价都需训练学习器，计算开销很大，因此算法设置了停止条件控制参数<span class="math inline">\(T\)</span>。然而，整个LVW算法是基于拉斯维加斯方法框架，若初始特征数很多（即<span class="math inline">\(|A|\)</span>很大）、<span class="math inline">\(T\)</span>设置较大，则算法可能运行很长时间都达不到停止条件，即若有运行时间限制，则可能无法产生解。</p>
<p><strong>拉斯维加斯方法与蒙特卡洛方法的主要区别</strong>：</p>
<ul>
<li>若有时间限制，则拉斯维加斯方法或者给出满足条件的解，或者不给出解；而蒙特卡洛方法一定会给出解，虽然给出的解未必满足要求；</li>
<li>若无时间限制，两者都能给出满足要求的解。</li>
</ul>
<h2 id="嵌入式选择与l_1正则化">4. 嵌入式选择与<span class="math inline">\(L_1\)</span>正则化</h2>
<p>在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显区别；与此不同，嵌入式特征选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。</p>
<p>给定数据集<span class="math inline">\(D = \{(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots,(\pmb{x}_m,y_m)\}\)</span>，其中<span class="math inline">\(\pmb{x} \in \mathbf{R}^d, y \in \mathbf{R}\)</span>。这里考虑最简单的线性回归模型，以平方误差为损失函数，则优化目标为 <span class="math display">\[\min_{\pmb{w}} \sum_{i=1}^m (y_i - \pmb{w}^T \pmb{x}_i)^2\]</span> 当样本特征很多，而样本数相对较少时，上式很容易陷入过拟合。为了缓解过拟合问题，在上式中加入正则项，若使用<span class="math inline">\(L_2\)</span>范数正则化，则有 <span class="math display">\[\min_{\pmb{w}} \sum_{i=1}^m (y_i - \pmb{w}^T \pmb{x}_i)^2 + \lambda ||\pmb{w}||_2^2\]</span> 其中正则化参数<span class="math inline">\(\lambda &gt; 0\)</span>，上式称为“岭回归”（ridge regression）。通过引入<span class="math inline">\(L_2\)</span>范数正则化，的确能显著降低过拟合的风险。<br>
若采用<span class="math inline">\(L_1\)</span>范数，则有 <span class="math display">\[\min_{\pmb{w}} \sum_{i=1}^m (y_i - \pmb{w}^T \pmb{x})^2 + \lambda ||\pmb{w}||_1\]</span> 其中正则化系数<span class="math inline">\(\lambda &gt; 0\)</span>，上式称为LASSO（Least Absolute Shrinkage and Selection Operator），直译为“最小绝对收缩选择算子”。<br>
<span class="math inline">\(L_1\)</span>范数和<span class="math inline">\(L_2\)</span>范数正则化有助于降低过拟合风险，<span class="math inline">\(L_1\)</span>范数还有一个额外的好处：<span class="math inline">\(L_1\)</span>范数比<span class="math inline">\(L_2\)</span>范数更易于获得“稀疏”（sparse）解，即它求得的<span class="math inline">\(\pmb{w}\)</span>会有更少的非零分量。</p>
<p><strong>事实上，对<span class="math inline">\(\pmb{w}\)</span>施加“系数约束”（即希望非零分量尽可能少）最自然的是使用<span class="math inline">\(L_0\)</span>范数，但<span class="math inline">\(L_0\)</span>范数不连续，难以优化求解，因此常用<span class="math inline">\(L_1\)</span>范数来近似</strong>。</p>
<div class="figure">
<img src="http://p35icynkw.bkt.clouddn.com/屏幕快照%202018-03-22%20下午1.49.53.png">

</div>
<p>分别引入<span class="math inline">\(L_1\)</span>和<span class="math inline">\(L_2\)</span>的目标函数的解要在平方误差项与正则化项之间折中，即出现在图中平方误差项等值线与正则化项等值线相交处。从上图可以看出，采用<span class="math inline">\(L_1\)</span>范数时平方误差项等值线与正则化项等值线的交点常出现在坐标轴上，即<span class="math inline">\(w_1\)</span>或<span class="math inline">\(w_2\)</span>为0；而在采用<span class="math inline">\(L_2\)</span>范数时，两者的焦点常出现在某个象限中，即<span class="math inline">\(w_1\)</span>和<span class="math inline">\(w_2\)</span>均非0。<strong>采用<span class="math inline">\(L_1\)</span>范数比<span class="math inline">\(L_2\)</span>范数更易于得到稀疏解</strong>。</p>
<p><span class="math inline">\(\pmb{w}\)</span>取得稀疏解意味着初始的<span class="math inline">\(d\)</span>个特征中仅有对应着<span class="math inline">\(\pmb{w}\)</span>的非零分量的特征才会出现在最终模型中，于是，求解<span class="math inline">\(L_1\)</span>范数正则化的结果是得到了仅采用一部分初始特征的模型。基于<span class="math inline">\(L_1\)</span>正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，同时完成。</p>
<p><span class="math inline">\(L_1\)</span>正则化问题的求解可使用<strong>近端梯度下降（Proximal Gradient Descent，PGD）</strong>。具体地说，令<span class="math inline">\(\nabla\)</span>表示微分算子，对优化目标 <span class="math display">\[\min_{\pmb{x}} f(\pmb{x}) + \lambda ||\pmb{x}||_1 \tag{1}\]</span> 若<span class="math inline">\(f(\pmb{x})\)</span>可导，且<span class="math inline">\(\nabla f\)</span>满足L-Lipschitz条件，即存在常数<span class="math inline">\(L &gt; 0\)</span>使得 <span class="math display">\[||\nabla f(\pmb{x}&#39;) - \nabla f(\pmb{x})||_2^2 \leq L||\pmb{x}&#39; - \pmb{x}||_2^2 \quad (\forall \pmb{x},\pmb{x}&#39;)\]</span> 则在<span class="math inline">\(\pmb{x}_k\)</span>附近可将<span class="math inline">\(f(\pmb{x})\)</span>通过二阶泰勒展开式近似为 <span class="math display">\[\begin{align}
    \hat{f}(\pmb{x}) &amp; \simeq f(\pmb{x}_k) + &lt;\nabla f(\pmb{x}_k),\pmb{x} - \pmb{x}_k&gt; + \frac{L}{2} ||\pmb{x} - \pmb{x}_k||^2 \nonumber \\
    &amp; = \frac{L}{2} ||\pmb{x} - (\pmb{x}_k - \frac{1}{L} \nabla f(\pmb{x}_k))||_2^2 + const \nonumber
\end{align}\]</span> 其中<span class="math inline">\(const\)</span>是与<span class="math inline">\(\pmb{x}\)</span>无关的常数，<span class="math inline">\(&lt;·,·&gt;\)</span>表示内积，显然，上式的最小值在如下<span class="math inline">\(\pmb{x}_{k+1}\)</span>获得： <span class="math display">\[\pmb{x}_{k+1} = \pmb{x}_k - \frac{1}{L} \nabla f(\pmb{x}_k)\]</span> 若通过梯度下降法对<span class="math inline">\(f(\pmb{x})\)</span>进行最小化，则每一步梯度下降迭代实际上等价于最小化二次函数<span class="math inline">\(\hat{f}(\pmb{x})\)</span>。将这个思想推广到公式（1），则能类似地得到其每一步迭代应为 <span class="math display">\[\pmb{x}_{k+1} = \arg \min_{\pmb{x}} \frac{L}{2} ||\pmb{x} - (\pmb{x}_k - \frac{1}{L} \nabla f(\pmb{x}_k))||_2^2 + \lambda ||\pmb{x}||_1\]</span> 即每一步对<span class="math inline">\(f(\pmb{x})\)</span>进行梯度下降迭代的同时考虑<span class="math inline">\(L_1\)</span>范数最小化。<br>
对于上式，可先计算<span class="math inline">\(\pmb{z} = \pmb{x}_k - \frac{1}{L} \nabla f(\pmb{x}_k)\)</span>，然后求解 <span class="math display">\[\pmb{x}_{k+1} = \arg \min_{\pmb{x}} \frac{L}{2} ||\pmb{x} - \pmb{z}||_2^2 + \lambda ||\pmb{x}||_1\]</span> 令<span class="math inline">\(x^i\)</span>表示<span class="math inline">\(\pmb{x}\)</span>的第<span class="math inline">\(i\)</span>个分量，将上式按分量展开可看出，其中不存在<span class="math inline">\(x^i x^j (i \neq j)\)</span>这样的项，即<span class="math inline">\(\pmb{x}\)</span>的各分量互不影响，于是上式有闭式解： <span class="math display">\[x_{k+1}^i = 
    \begin{cases}
        z^i - \lambda / L, \quad \lambda / L &lt; z^i; \\
        0, \qquad \qquad |z^i| \leq \lambda / L; \\
        z^i + \lambda / L, \quad z^i &lt; \lambda / L.
    \end{cases}\]</span></p>
<p>其中<span class="math inline">\(x_{k+1}^i\)</span>与<span class="math inline">\(z_i\)</span>分别表示是<span class="math inline">\(\pmb{x}_{k+1}\)</span>与<span class="math inline">\(z\)</span>的第<span class="math inline">\(i\)</span>个分量。因此，通过PGD能使LASSO和其他基于<span class="math inline">\(L_1\)</span>范数最小化的方法得以快速求解。</p>
<p><strong>闭式解推导</strong>： 这里采用soft thresholding算法：<br>
对于<span class="math inline">\(\pmb{x}\)</span>的第<span class="math inline">\(i\)</span>个分量<span class="math inline">\(x^i\)</span>: <span class="math display">\[\begin{equation}
    f(x^i) = \frac{L}{2} ||x^i - z^i||^2 + \lambda ||x^i||_1 \nonumber \\
    \partial f(x^i) = L(x^i - z^i) + \lambda ||x^i||_1
\end{equation}\]</span> 若<span class="math inline">\(x_{k+1}^i\)</span>为<span class="math inline">\(f(x)\)</span>的最小值，应满足<span class="math inline">\(0 \in \partial f(x_{k+1}^i)\)</span>，有如下推导： <span class="math display">\[\begin{equation}
    0 \in \partial f(x_{k+1}^i) \nonumber \\
    0 \in L(x^i - z^i) + \partial(\lambda ||x_{k+1}^i||_1) \nonumber \\
    (z^i - x^i) \in \partial (\frac{\lambda}{L} ||x_{k+1}^i||_1)
\end{equation}\]</span> 根据<span class="math inline">\(x_{k+1}^i\)</span>与0的关系分为三种情况：</p>
<ol style="list-style-type: decimal">
<li>若<span class="math inline">\(x_{k+1}^i &gt; 0\)</span>，则<span class="math inline">\(\partial (\frac{\lambda}{L} ||x_{k+1}^i||_1) = \frac{\lambda}{L}\)</span>，所以有<span class="math inline">\(x_{k+1}^i = z^i - \lambda / L\)</span>，此时<span class="math inline">\(\lambda/L &lt; z^i\)</span>；</li>
<li>若<span class="math inline">\(x_{k+1}^i &lt; 0\)</span>，则<span class="math inline">\(\partial (\frac{\lambda}{L} ||x_{k+1}^i||_1) = -\frac{\lambda}{L}\)</span>，所以有<span class="math inline">\(x_{k+1}^i = z^i + \lambda / L\)</span>，此时<span class="math inline">\(\lambda/L &gt; z^i\)</span>；</li>
<li>若<span class="math inline">\(x_{k+1}^i = 0\)</span>，则<span class="math inline">\(\partial (\frac{\lambda}{L} ||x_{k+1}^i||_1) = [-\frac{\lambda}{L},\frac{\lambda}{L}]\)</span>，所以有<span class="math inline">\(x_{k+1}^i = 0\)</span>，此时<span class="math inline">\(|z^i| \leq \lambda / L\)</span>；</li>
</ol>
<p>所以有闭式解 <span class="math display">\[x_{k+1}^i = 
    \begin{cases}
        z^i - \lambda / L, \quad \lambda / L &lt; z^i; \\
        0, \qquad \qquad |z^i| \leq \lambda / L; \\
        z^i + \lambda / L, \quad z^i &lt; \lambda / L.
    \end{cases}\]</span></p>
<p>其他部分的推导还有一些疑惑，只是简单的记一下，<a href="https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf" target="_blank" rel="noopener">PGD参考</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/预处理/" rel="tag"># 预处理</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/21/机器学习/度量学习/" rel="next" title="度量学习">
                <i class="fa fa-chevron-left"></i> 度量学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/24/机器学习/稀疏表示与字典学习/" rel="prev" title="稀疏表示与字典学习">
                稀疏表示与字典学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Daftar Isi
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Ikhtisar
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">silen Zhou</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">47</span>
                    <span class="site-state-item-name">posting</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    <span class="site-state-item-count">22</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#特征选择"><span class="nav-number">1.</span> <span class="nav-text">特征选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#子集搜索与评价"><span class="nav-number">1.1.</span> <span class="nav-text">1. 子集搜索与评价</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#子集搜索"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1. 子集搜索</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#子集评价"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2. 子集评价</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#过滤式选择"><span class="nav-number">1.2.</span> <span class="nav-text">2. 过滤式选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#包裹式选择"><span class="nav-number">1.3.</span> <span class="nav-text">3. 包裹式选择</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#嵌入式选择与l_1正则化"><span class="nav-number">1.4.</span> <span class="nav-text">4. 嵌入式选择与\(L_1\)正则化</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">silen Zhou</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Tema &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.2</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.0.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.0.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.2"></script>



  



	





  





  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

</body>
</html>
